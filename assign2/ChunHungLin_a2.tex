\documentclass[12pt]{article}

% packages
\usepackage[margin=1in]{geometry}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{subfig}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{cleveref}
\usepackage{listings}
\usepackage[makeroom]{cancel}
\usepackage[toc,page]{appendix}
\usepackage{enumerate}
\usepackage{csvsimple}
\usepackage{longtable}
\usepackage{pgfplotstable}
\usepackage{booktabs}

\setlength{\parindent}{1em}
\setlength{\parskip}{.3em}

% \newcommand{\N}{\mathbb{N}}
% \newcommand{\Z}{\mathbb{Z}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
% \newcommand*\descriptionlabel[1]{\hspace\leftmargin$#1$}

\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
%If you want to title your bold things something different just make another thing exactly like this but replace "problem" with the name of the thing you want, like theorem or lemma or whatever
 
\begin{document}
 
%\renewcommand{\qedsymbol}{\filledbox}
%Good resources for looking up how to do stuff:
%Binary operators: http://www.access2science.com/latex/Binary.html
%General help: http://en.wikibooks.org/wiki/LaTeX/Mathematics
%Or just google stuff
 
\title{DD2434 Machine Learning, Advanced Course Assignment 2}
\author{Lin Chun Hung, chlin3@kth.se}
\maketitle
%2.1
\begin{problem}{2.1.1}
    Yes, I have read it.
\end{problem}
\begin{problem}{2.1.2}
    I have asked problem formulations on Slacks. All of them were answered by Jens
    and TAs.
\end{problem}
\begin{problem}{2.1.2}
    No. I did not discuss solutions with anybody.
\end{problem}
%2.2
\begin{problem}{2.2.4}
    No
\end{problem}
\begin{problem}{2.2.5}
    Yes
\end{problem}
\begin{problem}{2.2.6}
\begin{align*}
    A = \{ \mu_{r,c}, 
            \mu_{(r-1 ~\text{mod}~ R,c)},
            \mu_{(r, c-1 ~\text{mod}~ C)},
            \mu_{(r, c+1 ~\text{mod}~ C)},
            \mu_{(r+1 ~\text{mod}~ R,c)}
        \}    
\end{align*}
\end{problem}
\begin{problem}{2.2.7}
    No
\end{problem}
\begin{problem}{2.2.8}
    No
\end{problem}
\begin{problem}{2.2.9}
\begin{align*}
    B = \{ Z^n_m: m \in [M], n \in [N]\} \cup \{ C^n: n \in [N]\}
\end{align*}
\end{problem}
\pagebreak

%2.3
\begin{problem}{2.3.10}
The implementation of this part is in Listing \ref{impl:Sec23}.

Define the evidence of subtree $L(T_u)$ as:
\begin{align*}
    s(u, i) = p(X_{L(T_u)} \mid X_u = i)
\end{align*}
where $L(T_u)$ is the observed leaves under the node $u$.

Consider a node $u$ has the childen $\{ c^{u}_1, c^{u}_2, ..., c^{u}_n\}$
\begin{align*}
    s(u, i) &= p(X_{L(T_u)} \mid X_u = i) \\
    &= \sum_{j_1, j_2, ...,j_n} p(X_{L(T_u)}, X_{c^{u}_1} = j_1, ..., X_{c^{u}_n}=j_n \mid X_u = i) \\
    &= \sum_{j_1, j_2, ...,j_n} 
        p(X_{L(T_u)} \mid X_{c^{u}_1} = j_1, ..., X_{c^{u}_n}=j_n, X_u = i)
        p(X_{c^{u}_1} = j_1, ..., X_{c^{u}_n}=j_n \mid X_u = i) \\
    &= \sum_{j_1, j_2, ...,j_n} 
        p(X_{L(T_u)} \mid X_{c^{u}_1} = j_1, ..., X_{c^{u}_n}=j_n)
        \prod_{l=1}^{N}p(X_{c^{u}_l} = j_l \mid X_u = i) \\
    &= \sum_{j_1, j_2, ...,j_n} 
        \prod_{m=1}^{N}p(X_{L(T_{c^{u}_l})}\mid X_{c^{u}_m} = j_m)
        \prod_{l=1}^{N}p(X_{c^{u}_l} = j_l \mid X_u = i) \\
    &= \prod_{l=1}^{N} [
        \sum_{j_l}p(X_{L(T_{c^{u}_l})}\mid X_{c^{u}_l} = j_l)p(c^{u}_l = j_l \mid X_u = i)] \\ 
        \numberthis \label{eq:Q23-recusive}
    &= \prod_{l=1}^{N} [
        \sum_{j_l}s(c^{u}_l, j_l)p(c^{u}_l = j_l \mid X_u = i)]  \\
    &= \prod_{l=1}^{N} [s(u, c^{u}_l, i)]
\end{align*}

In the implementation, the value of each $s(u, c^{u}_l, i)$ is saved to avoid repeat
calculation.

In eq \eqref{eq:Q23-recusive}, we can see that $s(u, i)$ has a recursive relation
to its childen. Therefore, we use DFS to traversal the tree in order to calculate
the value of $s(u, i)$.

At the leaf node $l$, we have $s(l, i) = \mathbb{I}(X_l = i)$ obviously.
\end{problem}

\begin{problem}{2.3.11}
The implmentation result is in Table \ref{tbl:Q23-result}.
\begin{table}
    \centering
    \pgfplotstabletypeset[
        col sep = comma,
        string replace*={_}{\textsubscript},
        every head row/.style={before row=\toprule,after row=\midrule},
        every last row/.style={after row=\bottomrule},
        display columns/0/.style={string type,column name={Tree}}
        ]{src/sec2_3/result_sec2_3.csv}
    \caption{The values of $p(\beta \mid T, \Theta)$}\label{tbl:Q23-result}
\end{table} 
\end{problem}
\pagebreak

% 2.5
\begin{problem}{2.5.15}

We have:
\begin{align*}
    s(u, i) = p(X_{L(T_u)} \mid X_u = i)
\end{align*}
$s(u,i)$ can be regarded as the evidence of the leaves of subtree $T_u$ given that
the value of this vertex $u$ is $i$.

Given the tree structure and the parameters of each vertex, the posterior is:
\begin{align*}
    p(X\mid \beta) = p(X_r\mid \beta)\prod_{v\in V(T)} p(X_v\mid X_{pa(v)}, \beta)
\end{align*}
where $X_r$ is the value of root.

The sampling distribution of the value of root is:
\begin{align*}
    p(X_r = i\mid \beta) &= \frac{p(X_r = i, \beta)}{p(\beta)} \\
    &= \frac{p(\beta \mid X_r =i)p(X_r =i)}{p(\beta)} \\
    &= \frac{s(r,i)p(X_r =i)}{p(\beta)} \\
    &= \frac{s(r,i)p(X_r =i)}{p(\beta)} \\
    &= \frac{s(r,i)p(X_r =i)}{\sum_{i}s(r,i)p(X_r =i)} \\
\end{align*}

Here we would like to introduce some symbols. 
$\beta^{-}_{v} = \{X_{v^{-}}: v^{-} \in L(T_v)\}$ is the values of the leaves
under the subtree $T_v$.
$\beta^{+}_{v} = \{X_{v^{+}}: v^{+} \in V(T) \setminus L(T_v)\}$ is the values of
the leaves not under the subtree $T_v$.

Then, the sampling distribution of vertex except the root node is given by:
\begin{align*}
    p(X_v = i\mid X_{pa(v)}, \beta)
    &= p(X_v = i\mid X_{pa(v)}, \beta^{-}_{v}, \beta^{+}_{v}) \\
    &= p(X_v = i\mid X_{pa(v)}, \beta^{-}_{v}, \cancel{\beta^{+}_{v}}) && \text{(By d-speration rule)}\\
    &= \frac{p(X_v = i, X_{pa(v)}, \beta^{-}_{v})}{p(X_{pa(v)}, \beta^{-}_{v})} \\
    &= \frac{p(\beta^{-}_{v}\mid X_v = i, \cancel{X_{pa(v)}})p(X_v = i, X_{pa(v)})}
        {p(X_{pa(v)}, \beta^{-}_{v})} && \text{(By d-speration rule)}\\
    &= \frac{p(\beta^{-}_{v}\mid X_v = i)p(X_v = i\mid X_{pa(v)})p(X_{pa(v)})}
        {p(\beta^{-}_{v}\mid X_{pa(v)})p(X_{pa(v)})} \\
    &= \frac{s(v,i)p(X_v = i\mid X_{pa(v)})}
            {\sum_{i}s(v,i)p(X_v = i\mid X_{pa(v)})}
\end{align*}

Therefore, we should first compute $s(u, i)$ for each vertex and each value 
from bottom to top and then sampling from the posterior from top to bottom
recursively.
\begin{align}
    X_r &\sim \frac{s(r,i)p(X_r =i)}{\sum_{i}s(r,i)p(X_r =i)} \label{eq:Q25-post_root} \\ 
    X_v &\sim \frac{s(v,i)p(X_v = i\mid X_{pa(v)}^{s})}
                {\sum_{j}s(v,j)p(X_v = j\mid X_{pa(v)}^{s})} \label{eq:Q25-post_node}
\end{align}
\end{problem}

\begin{problem}{2.5.16}
The implementation of this part is in Listing \ref{impl:Sec25}.

To make the implementation easier, this implementation is modified from the result
of 2.3.10. We calculate once the likelihood of the tree $p(\beta \mid T, \Theta)$ and
then save $\{s(u, i) : u \in V(T), i \in [K]\}$. And then use equations 
\eqref{eq:Q25-post_root} and \eqref{eq:Q25-post_node} to calculate the 
categorical distribution parameters for sampling.
\end{problem}

\begin{problem}{2.5.17}
The sampling result is in Table \ref{tbl:Q25-result}

\begin{table}
    \centering
    \pgfplotstabletypeset[
        col sep = comma,
        precision=6,
        every head row/.style={before row=\toprule,after row=\midrule},
        every last row/.style={after row=\bottomrule},
        % columns/Probability/.style={column name=$$}
        % display columns/0/.style={string type,column name={}}
        ]{src/sec2_5/result_sec2_5.csv}
    \caption{The values of $p(X_v = i\mid X_{pa(v)}, \beta)$}\label{tbl:Q25-result}
\end{table} 
\end{problem}


\pagebreak
% 2.6
\begin{problem}{2.6.18}
Notices that the each observation is the sum of two Gaussian random variables.
Therefore, we can first estimated the parameters of the observation model of 
a pair of players and then solve the linear equations to get each model parameters.
\begin{align*}
    p(x_m^{(n,n';r)}\mid z_m^{(n,n';r)} = i) = \mathcal{N}(x_m^{(n,n';r)} 
            \mid \mu^{n}_{(i,m)} + \mu^{n'}_{(i,m)}, 2\beta)
\end{align*}
where $z_m^{(n,n';r)} = i$ means in $r$:th round the pair of player $(n, n')$ visting 
subfield $s = (i, m)$ at sequence index $m$ and $z$ means hidden state.

First we will derive an EM algorithm for a HMM with Gaussian observation model 
with R observation data and then extend it to our case. This model can be considered
for one pair of players.

Consider the joint probability of a HMM where the initial state probaility and 
the transition matrix are given and the observation model is a conditional
Gaussian.
\begin{align*}
    p(z^{1:R}_{1:M}, x^{1:R}_{1:M}) &= \prod_{r=1}^{R} \biggl(
            p(z^r_1) \prod_{m=2}^{M}p(z^r_m\mid z^r_{m-1})
            \prod_{m=1}^{M}p(x^r_m\mid z^r_m) \biggl)\\
    p(x^r_m\mid z^r_m= i) &= \mathcal{N}(x^r_m \mid \mu_{(i,m)}, \Sigma_{(i,m)})
\end{align*}
where $x^{1:R}_{1:M}$ means R observation sequences indexed from 1 to M and
$z^r_m= i$ means the pair of player was visting subfield $(i, m)$ at round $R$.
Note that $z^r_m$ can be only 1 or 0 in our case.

The expected complete log likelihood (Q term):
\begin{align*}
    Q(\theta, \theta^{(t)}) &= \mathbb{E}_{p(Z\mid X, \theta^{(t)})}[
        \log p(z^{1:R}_{1:M}, x^{1:R}_{1:M})]
\end{align*}
where $\theta$ is the parameter set for this HMM model.

We can see that $\theta$ is given by:
\begin{align*}
    \theta = \{\bm{\pi}, \bm{A}, \bm{\mu}, \bm{\Sigma}\}
\end{align*}
where $\bm{\pi}$ is the initial state probability vector, $\bm{A}$ is the transition
matrix, $\bm{\mu} = \{ \mu_s: s \in S\}$ and $\bm{\Sigma} = \{ \Sigma_s: s \in S\}$.

We can write down the $Q(\theta, \theta^{t})$ by combining informations.
\begin{align*}
    Q(\theta, \theta^{t}) &= 
    \mathbb{E}_{p(Z\mid X, \theta^{t})}[ \sum_{r=1}^{R} \biggl(
        \log p(z^r_1) 
        + \sum_{m=2}^{M} \log p(z^r_m \mid z^r_{m-1}) 
        + \sum_{m=1}^{M} \log p(x^r_m\mid z^r_m)\biggl)] \\
    &= \mathbb{E}[\sum_{r=1}^{R} \log p(z_1)] 
        + \mathbb{E}[\sum_{r=1}^{R} \sum_{m=2}^{M} \log p(z^r_m \mid z^r_{m-1})] 
        + \mathbb{E}[\sum_{r=1}^{R} \sum_{m=1}^{M} \log p(x^r_m\mid z^r_m)] \\
    &= Q_{\pi}(\bm{\pi}) + Q_{A}(\bm{A}) + Q_{\mu, \Sigma}(\bm{\mu}, \bm{\Sigma})
\end{align*}
When $\bm{\pi}$ and $\bm{A}$ are given, $Q_{\pi}(\bm{\pi})$ and $Q_{A}(\bm{A})$ 
are not change in each iteration. Therefore, we only need to maximize 
$Q_{\mu, \Sigma}(\bm{\mu}, \bm{\Sigma})$ to update $\bm{\mu}$ and $\bm{\Sigma}$.

Let's go back to see the details of $Q_{\mu, \Sigma}(\bm{\mu}, \bm{\Sigma})$ 
\begin{align*}
    Q_{\mu, \Sigma}(\bm{\mu}, \bm{\Sigma}) 
    &= \mathbb{E}_{p(Z\mid X, \theta^{(t)})}[
            \sum_{r=1}^{R} \sum_{m=1}^{M} \log p(x^r_m\mid z^r_m)] \\
    &= \mathbb{E}_{p(Z\mid X, \theta^{(t)})}[
                \sum_{r=1}^{R} \sum_{m=1}^{M} \sum_{i=0}^{1} \mathbb{I}(z^r_m = i)\log p(x^r_m\mid z^r_m = i)] \\
    &= \sum_{Z} p(Z\mid X, \theta^{(t)})
            \sum_{r=1}^{R} \sum_{m=1}^{M} \sum_{i=0}^{1} \mathbb{I}(z^r_m = i)\log p(x^r_m\mid z^r_m = i) \\
    &= \sum_{r=1}^{R} \sum_{m=1}^{M} \sum_{i=0}^{1} p(z^r_m = i\mid X, \theta^{(t)})\log p(x^r_m\mid z^r_m = i) \\
    &= \sum_{r=1}^{R} \sum_{m=1}^{M} \sum_{i=0}^{1} p(z^r_m = i\mid x^{r}_{1:M}, \theta^{(t)})\log p(x^r_m\mid z^r_m = i) \\
    &= \sum_{r=1}^{R} \sum_{m=1}^{M} \sum_{i=0}^{1} \gamma^{r}_{m}(i)
        (-\frac{1}{2}\log 2\pi -\frac{1}{2}\log \Sigma_{(i,m)} - \frac{(x^r_m - \mu_{(i,m)})^2}{2\Sigma_{(i,m)}})
\end{align*}
where $\gamma^{r}_{m}(i) = p(z^r_m = i\mid x^{r}_{1:M}, \theta^{(t)})$ and it can be computed
by forward-backward algorithm.

In M step, we have:
\begin{align*}
    \bm{\mu}^{(t+1)} 
        &= \underset{\bm{\mu}}{\mathrm{argmin}} ~ Q_{\mu, \Sigma}(\bm{\mu}, \bm{\Sigma})  \\
    \bm{\Sigma}^{(t+1)} 
        &= \underset{\bm{\Sigma}}{\mathrm{argmin}} ~ Q_{\mu, \Sigma}(\bm{\mu}, \bm{\Sigma}) 
\end{align*}
By inspection, we can see that $Q_{\mu, \Sigma}(\bm{\mu}, \bm{\Sigma})$ 
is a convex function and therefore
we can find $\bm{\mu}^{(t+1)}$ and $\bm{\Sigma}^{(t+1)}$ 
by setting the first derivatives of $Q_{\mu, \Sigma}(\bm{\mu}, \bm{\Sigma})$
to zero.

\begin{flushleft}
    \textbf{\underline{Update $\mu_{(i,m)}$}}
\end{flushleft}

\begin{align*}
    \frac{\partial}{\partial \mu_{(i,m)}}Q_{\mu, \Sigma}(\bm{\mu}, \bm{\Sigma}) 
    &= \sum_{r=1}^{R} \gamma^{r}_{m}(i)(\frac{x^r_m - \mu_{(i,m)}}{\Sigma_{(i,m)}}) = 0 \\
    \mu_{(i,m)}
    &= \frac{\sum_{r=1}^{R}\gamma^{r}_{m}(i)x^r_m}{\sum_{r=1}^{R}\gamma^{r}_{m}(i)}
            \numberthis \label{eq:Q26-updateMu}
\end{align*}

\begin{flushleft}
    \textbf{\underline{Update $\Sigma_{(i,m)}$}}
\end{flushleft}

\begin{align*}
    \frac{\partial}{\partial {\Sigma_{(i,m)}}^{-1}}Q_{\mu, \Sigma}(\bm{\mu}, \bm{\Sigma}) 
    &= \sum_{r=1}^{R} \gamma^{r}_{m}(i)(\frac{1}{2{\Sigma_{(i,m)}}^{-1}} - \frac{1}{2}(x^r_m - \mu_{(i,m)})^2) = 0 \\
    \Sigma_{(i,m)}
    &= \frac{\sum_{r=1}^{R}\gamma^{r}_{m}(i)(x^r_m - \mu_{(i,m)})^2}
            {\sum_{r=1}^{R}\gamma^{r}_{m}(i)}  \numberthis \label{eq:Q26-updateSigma}
\end{align*}

For each pair of players, we estimate $\mu_s^{n,n'}$ and $\Sigma_s^{n,n'}$ by the
EM algorithm.
As what we have discussed in the begining, we have $\mu_s^{n,n'} = \mu_s^{n} + \mu_s^{n'}$.
Hence, $\mu_s^{n}$ can be obtained by solving the system of linear equations.

To obtain the value of $\beta$, we can use the following equation:
\begin{align*}
    \beta &= \frac{\sum_{s \in S}\sum_{n, n' \in [N]}\Sigma_s^{n,n'}}{2(2M)(N^2)} \\
    &= \frac{\sum_{s \in S}\sum_{n, n' \in [N]}\Sigma_s^{n,n'}}{4 M N^2} \\
\end{align*}
\end{problem}
% 2.6.19
\begin{problem}{2.6.19}
The implmentation of this part is in Appendix.

Here is the outline of procedure for each pair of players:
\begin{enumerate}[Step 1:]
    \item Initialize $\mu_{(i,m)}$ and $\Sigma_{(i,m)}$
    \item Calculate $\gamma^{r}_{m}(i)$ by forward-backward algorithm. \\
          $r$ is the index of round or index of data.
    \item Use $\gamma^{r}_{m}(i)$ to re-estimate $\mu_{(i,m)}$ and $\Sigma_{(i,m)}$
            using the eq \eqref{eq:Q26-updateMu} and \eqref{eq:Q26-updateSigma}
    \item If $\mu_{(i,m)}$ and $\Sigma_{(i,m)}$, do not converge go back to Step 2
\end{enumerate}

There are two tricks used in implmentation. The first trick is adding a scaling
factor in the alpha-pass and the beta-pass. It is because the result of gamma will 
underflow as $M$ become large. The second trick is set the lower bound to the 
emission probability. As the emission probability is a Gaussian distribution, 
sometime we could have a situation that the probability of emission became tiny,
say $1E-15$, and then the $\gamma^{r}_{m}(i)$ goes underflow immediately. To avoid
this situation, we should add a lower bound like $1E-6$ to the emission probability.
\end{problem}
\begin{problem}{2.6.20}
We set the length of sequecne $M$ to 5 and the number of players $N$ to 4.

We will go to discuss the parameter estimation result first and then we will 
discuss some behaviour in brief.

The true value of $\beta$ is $\pi^2 \approx 9.87$ and the estimated value of it
$\beta$ is 11.328.

The mean absolute derivation (MAE) and mean absolute percentage error (MAPE) 
between true value and the estimated value of $\mu^{n}_{s}$ are used to evaluate
the performance.

The MAE of $\hat{\mu}^{n}_{s}$ is 8.574 and the MAPE of $\hat{\mu}^{n}_{s}$ is 
66.5\%.

\end{problem}
\pagebreak

\begin{problem}{2.7.21}
Since vehicles are going from east to west monotonically, we can simplfy the
HMM models. The transition probabilites can be simplfied as 
$\{A^{k}_{i,j}: i, j \in [R]\}$ and the emission distribution can be simplfied as
$\{e^{k}_{r,(r+w)\mod R}: r \in [R], w \in \{-\Delta, -\Delta+1, ..., \Delta\}\}$
Also, the column index can be used as time index to define the order of the sequence.

Consider the joint probability of this model:
\begin{align*}
    p(C, X, Z \mid \pi, \lambda, A, e) 
    &= \prod_{n=1}^{N} p(C^{n}, X^n_{1:M}, Z^n_{1:M} \mid \pi, \lambda, A, e) \\
    &= \prod_{n=1}^{N} \prod_{k=1}^{K} [\pi_k p(X^n_{1:M}, Z^n_{1:M}\mid \lambda^k, A^{k}, e^{k})]^{c_{nk}} \\
\end{align*}
where $p(X^n_{1:M}, Z^n_{1:M}\mid \lambda^k, A^{k}, e^{k})$ is the $k$th HMM joint probability
for $n$th sequence data.


To the $k$th HMM model, $\lambda^k$ is the initial state probability vector,
$A^k$ is the transition matrix and $e^k$ is the emission matrix.

$c_{nk} = \mathbb{I}(C^n = k)$ is a indicate variable showing that $n$th sequence
belongs to $k$th group of cars.

The joint probability for the $n$th sequence data and the $k$th HMM is given by:
\begin{align*}
    p(X^n_{1:M}, Z^n_{1:M}\mid \lambda^k, A^{k}, e^{k})
    &= p(Z^n_1)\prod_{m=2}^{M}p(Z^n_m\mid Z^n_{m-1})\prod_{m=1}^{M}p(X^{n}_m\mid Z^n_m) \\
    &= \lambda^k_{Z^n_1}\prod_{m=2}^{M}A^k_{Z^n_{m-1},Z^n_m}\prod_{m=1}^{M}e^k_{Z^n_{m},X^n_m}\\
\end{align*}

Define the parameter set for this model as $\bm{\theta} = \{ \pi, \lambda, A, e\}$.

The complete data log-likelihood is given by:
\begin{align*}
    l_c(\bm{\theta}) &= \log [p(C, X, Z \mid \pi, \lambda, A, e)] \\
    &= \sum_{n=1}^{N} \sum_{k=1}^{K} 
        \log [(\pi_k p(X^n_{1:M}, Z^n_{1:M}\mid \lambda^k, A^{k}, e^{k}))^{c_{nk}}] \\
    &= \sum_{n=1}^{N} \sum_{k=1}^{K} 
        [c_{nk}\log \pi_k + c_{nk}\log p(X^n_{1:M}, Z^n_{1:M}\mid \lambda^k, A^{k}, e^{k})]  \\
    &= \sum_{n=1}^{N} \sum_{k=1}^{K}
        [c_{nk}\log \pi_k 
        + c_{nk} \log \lambda^k_{Z^n_1}
        + c_{nk} \sum_{m=2}^{M} \log A^k_{Z^n_{m-1},Z^n_m}
        + c_{nk} \sum_{m=1}^{M} \log e^k_{Z^n_{m},X^n_m}
        ]  \\
\end{align*}

With these results, we can write down the expected complete data log likelihood
(Q-term) easily:
\begin{align*}
    Q(\bm{\theta}, \bm{\theta}^{(t)}) 
    &= \mathbb{E}_{p(Z, C\mid X, \bm{\theta}^{(t)})}[l_c(\bm{\theta})]\\
    &= \sum_{n=1}^{N}  \sum_{k=1}^{K} \mathbb{E}[
            c_{nk}\log \pi_k + c_{nk} \log \lambda^k_{Z^n_1}
            + c_{nk} \sum_{m=2}^{M} \log A^k_{Z^n_{m-1},Z^n_m}
            + c_{nk} \sum_{m=1}^{M} \log e^k_{Z^n_{m},X^n_m}
            ]\\
    &= \sum_{n=1}^{N} \sum_{k=1}^{K} \biggl(
            \tau_{nk} \mathbb{E}[\log \pi_k] 
            + \tau_{nk} \mathbb{E}[\log \lambda^k_{Z^n_1}]
            + \tau_{nk} \sum_{m=2}^{M} \mathbb{E}[\log A^k_{Z^n_{m-1},Z^n_m}] \\
            & \hspace{1.5cm} 
            + \tau_{nk} \sum_{m=1}^{M} \mathbb{E}[\log e^k_{Z^n_{m},X^n_m}]
            \biggl)\\
    &= Q_{\pi^{(t)}}(\pi) + Q_{\lambda^{(t)}}(\lambda) + Q_{A^{(t)}}(A) + Q_{e^{(t)}}(e)
\end{align*}
where $\tau_{nk} = \mathbb{E}[c_{nk}] = p(C^n = k \mid X^n, \theta^{(t)})$ is the
responsibility that cluster $k$ takes for data sequence $n$.

There are 4 Q-terms in the above equation and therefore we can maximize 
$Q(\bm{\theta}, \bm{\theta}^{t})$ term-by-term.

Notice that
\begin{align*}
    \tau_{nk} &= p(C^n = k \mid X^n, \theta^{(t)})  \\
    & \propto p(C^n = k, X^n \mid \theta^{(t)}) \\
    & = p(C^n = k\mid \theta^{(t)})p(X^n \mid \theta^{(t)}, C^n = k) \\
    \tau_{nk}
    &= \frac{\pi_k^{(t)} p(X^n \mid \theta^{(t)}_k)}
            {\sum_{k'=1}^{K}\pi_{k'}^{(t)} p(X^n \mid \theta^{(t)}_{k'})}
        \numberthis \label{eq:Q27_tau}
\end{align*}
This result is the same as in the GMM EM algorithm.

\begin{align*}
    Q_{\pi^{(t)}}(\pi) &= \sum_{n=1}^{N} \sum_{k=1}^{K}
        \tau_{nk} \mathbb{E}_{p(Z\mid X, \bm{\theta}^{(t)}_k)}[\log \pi_k]  \\
    &=  \sum_{n=1}^{N} \sum_{k=1}^{K} 
        \tau_{nk} \log \pi_k\\
\end{align*}
The expectation operator does not take effect since $\pi_k$ is a parameter and 
not a random variable.

Maximize the above equation with respect to $\pi_k$ with the constraint $\sum_{k=1}^{K}\pi_k = 1$.
Then, the Lagrangian function is given by:
\begin{align*}
    \mathcal{L} = \sum_{n=1}^{N} \sum_{k=1}^{K} \tau_{nk} \log \pi_k - g(\sum_{k=1}^{K} \pi_k - 1)
\end{align*}
where $g$ is the Lagrange multiplier.

Calculate $\partial_{\pi_k}\mathcal{L} = 0$, we can find that
\begin{align*}
    \pi_k = \frac{\sum_{n=1}^{N} \tau_{nk}}{g}
\end{align*}
Make use of the constraint $\sum_{k=1}^{K}\pi_k = 1$, we can find the multiplier 
is given by:
\begin{align*} 
    \sum_{k=1}^{K}\pi_k &= \sum_{k=1}^{K} \frac{\sum_{n=1}^{N} \tau_{nk}}{g} = 1 \\
    g &= \sum_{n=1}^{N} \sum_{k=1}^{K}\tau_{nk} = \sum_{n=1}^{N} (1) = N
\end{align*}

Therefore the update equation for $\pi_k$ is given by:
\begin{equation}
    \pi_k^{(t+1)} = \frac{\sum_{n=1}^{N} \tau_{nk}}{N} \label{eq:Q27-update_pi_k}
\end{equation}

\begin{align*}
    Q_{\lambda^{(t)}}(\lambda) &= \sum_{n=1}^{N} \sum_{k=1}^{K}
        \tau_{nk} \mathbb{E}_{p(Z\mid X, \bm{\theta}^{(t)}_k)}[\log \lambda^k_{Z^n_1}] \\
    &= \sum_{n=1}^{N} \sum_{k=1}^{K} \tau_{nk} \mathbb{E}_{p(Z\mid X, \bm{\theta}^{(t)}_k)}
        [\sum_{r=1}^{R}\mathbb{I}(Z^n_1 = r)\log \lambda^k_{r}] \\
    &= \sum_{n=1}^{N} \sum_{k=1}^{K} \tau_{nk} \sum_{r=1}^{R} 
        p(Z^n_1 = r\mid X^n_{1:M}, \bm{\theta}^{(t)}_k) \log \lambda^k_{r} \\
    &= \sum_{n=1}^{N} \sum_{k=1}^{K} \tau_{nk} \sum_{r=1}^{R} 
        \gamma^{n;k}_{1}(r) \log \lambda^k_{r} \\
\end{align*}
where
\begin{align*}
    \gamma^{n;k}_{m}(r) = p(Z^n_m = r\mid X^n_{1:M}, \bm{\theta}^{(t)}_k)
\end{align*}
which can be found by the forward-backward algorithm.

Then we maximize $Q_{\lambda^{(t)}}(\lambda)$ with the constraint 
$\sum_{r=1}^{R} \lambda_r^k = 1$.

The Lagrangian function is given by:
\begin{align*}
    \mathcal{L} &= 
        \sum_{n=1}^{N} \sum_{k=1}^{K} \tau_{nk} \sum_{r=1}^{R} 
            \gamma^{n;k}_{1}(r) \log \lambda^k_{r} 
        - g(\sum_{r=1}^{R} \lambda_r^k - 1)\\
\end{align*}

Calculate $\partial_{\lambda^k_{r}}\mathcal{L} = 0$, 
use the same trick as in finding the update equation of $\pi_k$ and the fact that
$\sum_{r=1}^{R}\gamma^{n;k}_{1}(r) = 1$. We can get the update equation of 
$\lambda^k_{r}$:
\begin{equation}
    {\lambda^k_{r}}^{(t+1)} = \frac{\sum_{n=1}^{N}\tau_{nk} \gamma^{n;k}_{1}(r)}
                                   {\sum_{n=1}^{N}\tau_{nk}}
\end{equation}

For updating $A$:
\begin{align*}
    Q_{A^{(t)}}(A) &= \sum_{n=1}^{N} \sum_{k=1}^{K} \tau_{nk} \sum_{m=2}^{M} 
        \mathbb{E}_{p(Z\mid X, \bm{\theta}^{(t)}_k)}[\log A^k_{Z^n_{m-1},Z^n_m}] \\
    &= \sum_{n=1}^{N} \sum_{k=1}^{K} \tau_{nk} \sum_{m=2}^{M} 
        \mathbb{E}_{p(Z\mid X, \bm{\theta}^{(t)}_k)}
        [\sum_{i,j=1}^{R} \mathbb{I}(Z^n_{m-1} = i, Z^n_m = j) \log A^k_{i,j}] \\
    &= \sum_{n=1}^{N} \sum_{k=1}^{K} \tau_{nk} \sum_{m=2}^{M} 
        \sum_{i,j=1}^{R} p(Z^n_{m-1} = i, Z^n_m = j \mid X^n_{1:M}, \bm{\theta}^{(t)}_k) \log A^k_{i,j} \\
    &= \sum_{n=1}^{N} \sum_{k=1}^{K} \tau_{nk} \sum_{m=2}^{M} \sum_{i,j=1}^{R} \xi_{m}^{n;k}(i,j)\log A^k_{i,j}
\end{align*}
where
\begin{align*}
    \xi_{m}^{n;k}(i,j) = p(Z^n_{m-1} = i, Z^n_m = j \mid X^n_{1:M}, \bm{\theta}^{(t)}_k)
\end{align*}
We have the constraint $\sum_{j=1}^{R} A^k_{i,j} - 1$.
Therefore the Lagrangian function is given by:
\begin{align*}
    \mathcal{L} = 
        \sum_{n=1}^{N} \sum_{k=1}^{K} 
        \tau_{nk} \sum_{m=2}^{M} \sum_{i,j=1}^{R} \xi_{m}^{n;k}(i,j)\log A^k_{i,j}
        - g(\sum_{j=1}^{R} A^k_{i,j} - 1)
\end{align*}

Maximize the above equation with respect to $A^k_{i,j}$, we can get:
\begin{align*}
    A^k_{i,j}&= \frac{\sum_{n=1}^{N} \tau_{nk} \sum_{m=2}^{M} \xi_{m}^{n;k}(i,j)}
                     {\sum_{n=1}^{N} \tau_{nk} \sum_{m=2}^{M} \sum_{j=1}^{R} \xi_{m}^{n;k}(i,j)} \\
             &= \frac{\sum_{n=1}^{N} \tau_{nk} \sum_{m=2}^{M} \xi_{m}^{n;k}(i,j)}
                     {\sum_{n=1}^{N} \tau_{nk} \sum_{m=2}^{M} \gamma_{m-1}^{n;k}(i)} \\
             &= \frac{\sum_{n=1}^{N} \tau_{nk} \sum_{m=2}^{M} \xi_{m}^{n;k}(i,j)}
                     {\sum_{n=1}^{N} \tau_{nk} \sum_{m=1}^{M-1} \gamma_{m}^{n;k}(i)} \\
\end{align*}
The update equation of $A$ is:
\begin{equation}
    {A^k_{i,j}}^{(t+1)} = 
        \frac{\sum_{n=1}^{N} \tau_{nk} \sum_{m=2}^{M} \xi_{m}^{n;k}(i,j)}
             {\sum_{n=1}^{N} \tau_{nk} \sum_{m=1}^{M-1} \gamma_{m}^{n;k}(i)}
\end{equation}

Finally, we derive the update equation of $e$.
\begin{align*}
    Q_{e^{(t)}}(e) &= \sum_{n=1}^{N} \sum_{k=1}^{K} \tau_{nk} \sum_{m=1}^{M} 
        \mathbb{E}_{p(Z\mid X, \bm{\theta}^{(t)}_k)}[\log e^k_{Z^n_{m},X^n_m}] \\
    &= \sum_{n=1}^{N} \sum_{k=1}^{K} \tau_{nk} \sum_{m=1}^{M} 
        \mathbb{E}_{p(Z\mid X, \bm{\theta}^{(t)}_k)}
            [\sum_{i=1}^{R}\sum_{l=1}^{R}
            \mathbb{I}(Z^n_{m} = i)\mathbb{I}(X^n_{m} = l)\log e^k_{i,l}] \\
    &= \sum_{n=1}^{N} \sum_{k=1}^{K} \tau_{nk} \sum_{m=1}^{M} 
        [\sum_{i=1}^{R}\sum_{l=1}^{R} p(Z^n_{m} = i\mid X^{n}_{1:M}, \bm{\theta}^{(t)}_k)
        \mathbb{I}(X^n_{m} = l)\log e^k_{i,l}] \\
    &= \sum_{n=1}^{N} \sum_{k=1}^{K} \tau_{nk} \sum_{m=1}^{M} 
        [\sum_{i=1}^{R}\sum_{l=1}^{R} \gamma_{m}^{n;k}(i)
        \mathbb{I}(X^n_{m} = l)\log e^k_{i,l}] \\
\end{align*}
Here we assume that the number of observation states is $R$ since the noise can
be very large.

The Lagrangian function is:
\begin{align*}
    \mathcal{L} = 
        \sum_{n=1}^{N} \sum_{k=1}^{K} \tau_{nk} \sum_{m=1}^{M} 
            [\sum_{i=1}^{R}\sum_{l=1}^{R} \gamma_{m}^{n;k}(i)
            \mathbb{I}(X^n_{m} = l)\log e^k_{i,l}] 
        - g(\sum_{l'=1}^{R}e^k_{i,l'} - 1)\\
\end{align*}
Maximize the Lagrangian function and we can get:
\begin{equation}
    e^k_{i,l} = 
    \frac{\sum_{n=1}^{N}\tau_{nk} \sum_{m=1}^{M} \gamma_{m}^{n;k}(i)\mathbb{I}(X^n_{m} = l)}
        {\sum_{n=1}^{N}\tau_{nk} \sum_{m=1}^{M} \gamma_{m}^{n;k}(i)}
\end{equation}

These update equations can be reduced to one model N data case by setting $K = 1$.
\end{problem}
\pagebreak



% 2.8
\begin{problem}{2.8.24}
Consider the full joint distribution can be factorized as:
\begin{align*}
    p(X, Z, C, \Theta, \pi) = p(Z, C, \Theta, \pi \mid X)p(X)
\end{align*}
And the posterior $p(Z, C, \Theta, \pi \mid X)$ can be approximated by VI.

\medskip
By the graph, we can see the joint probability as:
\begin{align*}
    p(X, Z, C, \Theta, \pi) = p(X, Z \mid \Theta, C) p(C\mid \pi) p(\pi) p(\Theta)
\end{align*}
And the log probability:
\begin{align*}
    \log p(X, Z, C, \Theta, \pi) = 
    &\log p(X \mid Z, e, C) + \log p(Z \mid A, C) + \log p(C\mid \pi)  \\
    & + \log p(\pi) + \log p(A)  + \log p(e)
\end{align*}
From the question description, we can write down:
\begin{align*}
    p(\pi) &= \mathrm{Dir}(\pi_1, \pi_2, ..., \pi_K \mid \alpha_1, \alpha_2, ..., \alpha_K) \\
    p(A) &= \prod_{k,r} \mathrm{Dir}(A^k_{r,1}, A^k_{r,2}, ..., A^k_{r,R},\mid \alpha_A) \\
    p(B) &= \prod_{k,r} \mathrm{Dir}(e^k_{r,r-\Delta}, e^k_{r,r-\Delta +1}, ..., e^k_{r,r+\Delta}\mid \alpha_e) \\
    p(C\mid \pi) &= \prod_{n,k} \pi_k^{c_{nk}} \\
    p(Z\mid C, A) &= \prod_{n,k}(\lambda^k_{Z^n_{1}}
        \prod_{m=2}^{M} A^k_{Z^n_{m}Z^n_{m+1}})^{c_{nk}} \\
    p(X\mid Z, C, e) &= \prod_{n,k} (\prod_{m=1}^{M} e^k_{Z^n_{m}X^n_{m}})^{c_{nk}}
    % p(X, Z\mid \Theta, C) &= 
    %     \prod_{n,k}(\lambda^k_{Z^n_{1}}
    %     \prod_{m=2}^{M} A^k_{Z^n_{m}Z^n_{m+1}}
    %     \prod_{m=1}^{M} e^k_{Z^n_{m}X^n_{m}})^{c_{nk}}
\end{align*}
where $c_{nk}=1$ if sequence ~$X^n$ belongs to cluster k otherwise $c_{nk}=0$,
      $\lambda^k_{Z^n_{1}}$ is the initial-state probability,
      $A^k_{Z^n_{m}Z^n_{m+1}} = p(Z^n_{m+1}\mid Z^n_{m})$ is the transition probability,
  and $e^k_{Z^n_{m}X^n_{m}} = p(X^n_{m}\mid Z^n_{m})$ is the emission probability.

Assuming the following factorization.
\begin{align*}
    q(Z, C, A, e, \pi) &= q(Z)q(C)q(A)q(e)q(\pi)
\end{align*}


% Update Q(A)
\begin{flushleft}
\textbf{\underline{Update $q(A)$}}
\end{flushleft}

\begin{align*}
    \log q(A) &= \mathbb{E}_{q(Z)q(\pi)q(C)q(e)}[\log p(X, Z, C, \Theta, \pi)] + \text{const} \\
    &= \mathbb{E}_{q(Z)q(C)}[\log p(Z\mid A, C)] + \log p(A)+ \text{const} \\
    &= \mathbb{E}_{q(Z)q(C)}[\sum_{n,k}c_{nk}(\log\lambda^k_{Z^n_{1}} + \sum_{m=2}^{M}\log A^k_{Z^n_{m}Z^n_{m+1}})] 
        + \log p(A)+ \text{const} \\
    &= \sum_{n,k}\mathbb{E}_{q(C)}[c_{nk}]\mathbb{E}_{q(Z)}[\sum_{m=2}^{M}\log A^k_{Z^n_{m}Z^n_{m+1}}] 
        + \log p(A)+ \text{const} \\
\end{align*}
We look at the equation above term-by term:
\begin{align*}
    \log p(A) &= \sum_{k,r}\sum_{j=1}^{R} (\alpha_{A,j} - 1)\log A^k_{r,j} \\
    &= \sum_{k=1}^{K}\sum_{r=1}^{R}\sum_{j=1}^{R} (\alpha_{A,j} - 1)\log A^k_{r,j}
\end{align*}
\begin{align*}
    \mathbb{E}_{q(Z)}[\sum_{m=2}^{M}\log A^k_{Z^n_{m}Z^n_{m+1}}] &=
    \sum_{Z}q(Z)\sum_{m=2}^{M}\log A^k_{Z^n_{m}Z^n_{m+1}} \\
    &= \sum_{Z}q(Z)\sum_{m=2}^{M}\sum_{r=1}^{R}\sum_{j=1}^{R}\mathbb{I}(Z^n_{m}=r, Z^n_{m+1}=j)\log A^k_{r,j} \\
    &= \sum_{r=1}^{R}\sum_{j=1}^{R}\sum_{m=2}^{M}q(Z^n_{m}=r, Z^n_{m+1}=j)\log A^k_{r,j}
\end{align*}
For simplicity, define $r_{nk}$ as:
\begin{equation}
    r_{nk} = \mathbb{E}_{q(C)}[c_{nk}] 
\end{equation}
Therefore,
\begin{align*}
    &\sum_{n,k}\mathbb{E}_{q(C)}[c_{nk}]\mathbb{E}_{q(Z)}[\sum_{m=2}^{M}\log A^k_{Z^n_{m}Z^n_{m+1}}] \\
    &= \sum_{n,k} r_{nk} \sum_{r=1}^{R}\sum_{j=1}^{R}\sum_{m=2}^{M}q(Z^n_{m}=r, Z^n_{m+1}=j)\log A^k_{r,j} \\
    &= \sum_{k}\sum_{r}\sum_{j}(\sum_{n}\sum_{m=2}r_{nk} q(Z^n_{m}=r, Z^n_{m+1}=j))\log A^k_{r,j}
\end{align*}
Therefore, we can see that $q(A)$ is a Dirichlet Distribution.
\begin{align*}
    q(A) = \prod_{k,r} \mathrm{Dir}(A^k_{r,1}, A^k_{r,2}, ..., A^k_{r,R}
        \mid \tilde{\alpha_{A}}^k_{r,1},\tilde{\alpha_{A}}^k_{r,2},...,\tilde{\alpha_{A}}^k_{r,R}) \\
\end{align*}
with the parameters:
\begin{equation}
    \tilde{\alpha_{A}}^k_{r,j} = \alpha_{A,j} + \sum_{n=1}^{N}\sum_{m=2}^{M}r_{nk} q(Z^n_{m}=r, Z^n_{m+1}=j)
\end{equation}

% Update Q(e)
\begin{flushleft}
    \textbf{\underline{Update $q(e)$}}
\end{flushleft}
% Consider $q(e)$:
\begin{align*}
    \log q(e) &= \mathbb{E}_{q(Z)q(\pi)q(C)q(A)}[\log p(X, Z, C, \Theta, \pi)] + \text{const} \\
    &= \log p(e) + \mathbb{E}_{q(Z)q(C)}[\log p(X\mid Z, C, e)] + \text{const} \\
    &= \log p(e) 
        + \sum_{n,k}\mathbb{E}_{q(C)}[c_{nk}]\sum_{m=1}^{M}\mathbb{E}_{q(Z)}[\log e^k_{Z^n_{m}X^n_{m}}] 
        + \text{const} \\
\end{align*}
Term by Term:
\begin{align*}
    \log p(e) &= \sum_{k,r}\sum_{l=-\Delta}^{\Delta} (\alpha_{e,l} - 1)\log e^k_{r,r+l} \\
    &= \sum_{k=1}^{K}\sum_{r=1}^{R}\sum_{l=-\Delta}^{\Delta} (\alpha_{e,l} - 1)\log e^k_{r,r+l}
\end{align*}
We can use the same skill as what we do in calculating $q(A)$
\begin{align*}
    \mathbb{E}_{q(Z)}[\mathbb{I}(Z^n_{m}=i, X^n_{m}=r+l)] 
    &= \mathbb{E}_{q(Z)}[\mathbb{I}(Z^n_{m}=i)]\mathbb{I}(X^n_{m}=r+l) \\
    &= q(Z^n_{m}=i)\mathbb{I}(X^n_{m}=r+l)
\end{align*}
Then, we have:
\begin{align*}
    \mathbb{E}_{q(Z)}[\log e^k_{Z^n_{m}X^n_{m}}] 
    &= \mathbb{E}_{q(Z)}[\sum_{i,j}\mathbb{I}(Z^n_{m}=i, X^n_{m}=j)\log e^k_{i,j}] \\
    &= \mathbb{E}_{q(Z)}[\sum_{i,l}\mathbb{I}(Z^n_{m}=i, X^n_{m}=r+l)\log e^k_{i,r+l}] \\
    &= \sum_{i=1}^{R}\sum_{l=-\Delta}^{\Delta}q(Z^n_{m}=i)\mathbb{I}(X^n_{m}=r+l)\log e^k_{i,r+l} \\
\end{align*}
Therefore,
\begin{align*}
    &\sum_{n,k}\mathbb{E}_{q(C)}[c_{nk}]\sum_{m=1}^{M}\mathbb{E}_{q(Z)}[\log e^k_{Z^n_{m}X^n_{m}}]\\
    &= \sum_{n,k}r_{nk}\sum_{m=1}^{M}\sum_{i=1}^{R}\sum_{l=-\Delta}^{\Delta}q(Z^n_{m}=i)\mathbb{I}(X^n_{m}=r+l)\log e^k_{i,r+l} \\
    &= \sum_{k=1}^{K}\sum_{i=1}^{R}\sum_{l=-\Delta}^{\Delta}
        (\sum_{n=1}^{N}\sum_{m=1}^{M}r_{nk}q(Z^n_{m}=i)\mathbb{I}(X^n_{m}=r+l))\log e^k_{i,r+l}
\end{align*}
Obviously, $q(e)$ is also a Dirichlet distribution.
\begin{align*}
    q(e) = \prod_{k,r} \mathrm{Dir}(e^k_{r,r-\Delta}, e^k_{r,r-\Delta+1}, ..., e^k_{r,r+\Delta}
        \mid \tilde{\alpha_{e}}^k_{r,-\Delta},\tilde{\alpha_{e}}^k_{r,-\Delta+1},...,\tilde{\alpha_{e}}^k_{r,\Delta}) \\
\end{align*}
with the parameters:
\begin{equation}
    \tilde{\alpha_{e}}^k_{r,l} = \alpha_{e,l} + \sum_{n=1}^{N}\sum_{m=1}^{M}r_{nk} q(Z^n_{m}=i)\mathbb{I}(X^n_{m}=r+l)
\end{equation}

% Update Q(pi)
\begin{flushleft}
    \textbf{\underline{Update $q(\pi)$}}
\end{flushleft}

\begin{align*}
    \log q(\pi) &= \mathbb{E}_{q(Z)q(C)q(A)q(e)}[\log p(X, Z, C, \Theta, \pi)] + \text{const} \\
    &= \log p(\pi) + \mathbb{E}_{q(C)}[\log p(C \mid \pi)] + \text{const} \\
    &= \sum_{k}(\alpha_k - 1)\log \pi_k + \mathbb{E}_{q(C)}[\sum_{n,k}c_{nk}\log\pi_k] + \text{const} \\
    &= \sum_{k}(\alpha_k + \sum_{n}\mathbb{E}_{q(C)}[c_{nk}] - 1)\log \pi_k + \text{const} \\
    &= \sum_{k}(\alpha_k + \sum_{n}r_{nk} - 1)\log \pi_k + \text{const} \\
\end{align*}
Therefore, $q(\pi)$ is a Dirichlet distribution 
\begin{align*}
    q(\pi) = \mathrm{Dir}(\pi_1, \pi_2, ..., \pi_K \mid 
    \tilde{\alpha_{\pi}}_1,\tilde{\alpha_{\pi}}_2,...,\tilde{\alpha_{\pi}}_K)
\end{align*}
with parameters:
\begin{equation}
    \tilde{\alpha_{\pi}}_k = \alpha_k + \sum_{n=1}^{N}r_{nk}
\end{equation}
% Update Q(Z)
\begin{flushleft}
    \textbf{\underline{Update $q(Z)$}}
\end{flushleft}
\begin{align*}
    \log q(Z) &= \mathbb{E}_{q(\pi)q(C)q(A)q(e)}[\log p(X, Z, C, \Theta, \pi)] + \text{const} \\
    &= \mathbb{E}_{q(e)}[\log p(X \mid Z, C, e)] + \mathbb{E}_{q(A)}[\log p(Z \mid C, A)]
       + \text{const} \\
    &= \mathbb{E}_{q(e)q(C)}[\sum_{n,k}c_{nk}\sum_{m=1}^{M}\log e^k_{Z^n_m X^n_m}] \\
    &\hspace{0.5cm}  + \mathbb{E}_{q(A)q(C)}[\sum_{n,k}c_{nk}(\log\lambda^k_{Z^n_{1}} + \sum_{m=2}^{M}\log A^k_{Z^n_{m}Z^n_{m+1}})]
       + \text{const} \\
    &= \sum_{n,k}r_{nk}\sum_{m=1}^{M}\log \tilde{e}^k_{Z^n_m X^n_m}
        + \sum_{n,k}r_{nk}(\log\lambda^k_{Z^n_{1}} + \sum_{m=2}^{M}\log \tilde{A}^k_{Z^n_{m}Z^n_{m+1}})
        + \text{const} \\
    &= \sum_{n,k}r_{nk}(
        (\log\lambda^k_{Z^n_{1}} + \sum_{m=2}^{M}\log \tilde{A}^k_{Z^n_{m}Z^n_{m+1}}
        +\sum_{m=1}^{M} \log \tilde{e}^k_{Z^n_m X^n_m})
        + \text{const} \\
\end{align*}
where we define $\tilde{A}^k_{Z^n_{m}Z^n_{m+1}}$ as:
\begin{align*}
    \log \tilde{A}^k_{Z^n_{m}Z^n_{m+1}} &= \mathbb{E}_{q(A)}[\log A^k_{Z^n_{m}Z^n_{m+1}}] - \log Z_{A}\\
    &= \sum_{i,j}\mathbb{I}(Z^n_{m} = i, Z^n_{m+1}=j)\mathbb{E}_{q(A)}[\log A^k_{i,j}] - \log Z_{A}\\
    &= \sum_{i,j}\mathbb{I}(Z^n_{m} = i, Z^n_{m+1}=j)
    (\psi(\tilde{\alpha_{A}}^k_{i,j}) -\psi(\sum_{l=1}^{R}\tilde{\alpha_{A}}^k_{i,l}))- \log Z_{A}\\
    &= \psi(\tilde{\alpha_{A}}^k_{Z^n_{m},Z^n_{m+1}}) -\psi(\sum_{l=1}^{R}\tilde{\alpha_{A}}^k_{Z^n_{m},l}) - \log Z_{A}\\
    \tilde{A}^k_{Z^n_{m}Z^n_{m+1}} &= \frac{1}{Z_{A}}
    \exp[\psi(\tilde{\alpha_{A}}^k_{Z^n_{m},Z^n_{m+1}}) -\psi(\sum_{l=1}^{R}\tilde{\alpha_{A}}^k_{Z^n_{m},l})] \\
\end{align*}
Simularly, we can define $\tilde{e}^k_{Z^n_m X^n_m}$:
\begin{align*}
    % log b_tilde
    \log \tilde{e}^k_{Z^n_m X^n_m} 
    &= \psi(\tilde{\alpha_{e}}^k_{Z^n_{m},X^n_{m}}) -\psi(\sum_{l=-\Delta}^{\Delta}\tilde{\alpha_{e}}^k_{Z^n_{m},Z^n_{m}+l})
    - \log Z_{e}\\
    \tilde{e}^k_{Z^n_m X^n_m} &= \frac{1}{Z_{e}}
    \exp[\psi(\tilde{\alpha_{e}}^k_{Z^n_{m},X^n_{m}}) -\psi(\sum_{l=-\Delta}^{\Delta}\tilde{\alpha_{e}}^k_{Z^n_{m},Z^n_{m}+l})]\\
\end{align*}
where $\psi(.)$ is the digamma function as well as
$Z_{A}$ and $Z_{e}$ are normalization constants to make sure that $\tilde{A}^{k}_{i,j}$
and $\tilde{e}^{k}_{i,l}$ are stochastic matrices.

Take antilogarithm on $\log q(Z)$, we have:
\begin{align*}
    q(Z) = \frac{1}{Z_{*}}\prod_{n,k}(\lambda^k_{Z^n_{1}}
        \prod_{m=2}^{M}\tilde{A}^k_{Z^n_{m}Z^n_{m+1}}\prod_{m=1}^{M}\tilde{e}^k_{Z^n_m X^n_m})^{r_{nk}}
\end{align*}
where $Z_{*}$ is a normalization constant.

Consider:
\begin{align*}
    p(Z\mid X) &\approx q(Z) \\
    &= \frac{1}{Z_{*}}\prod_{n,k}p(Z^n_{1:M}, X^n_{1:M} \mid \tilde{A}^k, \tilde{e}^k)^{r_{nk}} \\
    &= \frac{\prod_{n,k}p(Z^n_{1:M}, X^n_{1:M} \mid \tilde{A}^k, \tilde{e}^k)^{r_{nk}}}
            {\prod_{n,k}p(X^n_{1:M} \mid \tilde{A}^k, \tilde{e}^k)^{r_{nk}}}
\end{align*}

In the above equation, we can consider that the numerator is a joint probability of
a mixture of HMMs. The dominator is the approximated data likelihood given 
the parameters $\tilde{A}$ and $\tilde{e}$.
The data likelihood can be found by forward algorithm of HMM. 

In updating $q(A)$ and $q(e)$, we can use 
the forward-backward algorithm to calculate $q(Z^n_{m}=i)$ and $q(Z^n_{m}=r, Z^n_{m+1}=j)$
because:
\begin{align*}
    q(Z^n_{m}=i) &\approx p(Z^n_{m}=i \mid X, \tilde{A}, \tilde{e}) \\
    q(Z^n_{m}=i, Z^n_{m+1}=j) &\approx p(Z^n_{m}=i, Z^n_{m+1}=j\mid X, \tilde{A}, \tilde{e})
\end{align*}

% Update q(C)
\begin{flushleft}
    \textbf{\underline{Update $q(C)$}}
\end{flushleft}
\begin{align*}
    \log q(C) &= \mathbb{E}_{q(Z)q(A)q(e)q(\pi)}[\log p(X, Z, C, \Theta, \pi)] + \text{const} \\
    &= \mathbb{E}_{q(\pi)}[\log p(C \mid \pi)]
        + \mathbb{E}_{q(Z)q(e)}[\log p(X \mid Z, C, e)]  \\
        &+ \mathbb{E}_{q(Z)q(A)}[\log p(Z \mid C, A)] + \text{const} \\
\end{align*}

The first term $\mathbb{E}_{q(\pi)}[\log p(C \mid \pi)]$:
\begin{align*}
    \mathbb{E}_{q(\pi)}[\log p(C \mid \pi)] 
    &= \mathbb{E}_{q(\pi)}[\sum_{n,k} c_{nk} \log \pi_k] \\
    &= \sum_{n,k} c_{nk}\mathbb{E}_{q(\pi)}[\log \pi_k] \\
    &= \sum_{n,k} c_{nk}(\psi(\tilde{\alpha_\pi}_k) - \psi(\sum_{j=1}^{K}\tilde{\alpha_\pi}_j)) \\
\end{align*}

The second term $\mathbb{E}_{q(Z)q(e)}[\log p(X \mid Z, C, e)]$:
\begin{align*}
    \mathbb{E}_{q(Z)q(e)}[\log p(X \mid Z, C, e)]
    &= \mathbb{E}_{q(Z)q(e)}[\sum_{n,k}c_{nk}\sum_{m=1}^{M}\log e^k_{Z^n_m X^n_m}] \\
    &= \sum_{n,k}c_{nk}\mathbb{E}_{q(Z)q(e)}[\sum_{m=1}^{M}\log e^k_{Z^n_m X^n_m}] \\
    &= \sum_{n,k}c_{nk}\mathbb{E}_{q(Z)}[\sum_{m=1}^{M}\log \tilde{e}^k_{Z^n_m X^n_m}]
\end{align*}

The third term $\mathbb{E}_{q(Z)q(A)}[\log p(Z \mid C, A)]$:
\begin{align*}
    \mathbb{E}_{q(Z)q(A)}[\log p(Z \mid C, A)]
    &= \mathbb{E}_{q(Z)q(A)}[
        \sum_{n,k}c_{nk}(\log\lambda^k_{Z^n_{1}} + \sum_{m=2}^{M}\log A^k_{Z^n_{m}Z^n_{m+1}})] \\
    &= \sum_{n,k}c_{nk}(\log\lambda^k_{Z^n_{1}} 
        + \mathbb{E}_{q(Z)q(A)}[\sum_{m=2}^{M}\log A^k_{Z^n_{m}Z^n_{m+1}}]) \\
    &= \sum_{n,k}c_{nk}(\log\lambda^k_{Z^n_{1}} 
        + \mathbb{E}_{q(Z)}[\sum_{m=2}^{M}\log \tilde{A}^k_{Z^n_{m}Z^n_{m+1}}]) \\
\end{align*}
Combining the second term and the third term, we can get an entropy-like term
of the mixture of HMMs. That term can be computed by a recursive algorithm.
Define:
\begin{align*}
    \tilde{H}_{nk}(q(Z)) &= -\mathbb{E}_{q(Z)}[
            \log\lambda^k_{Z^n_{1}} 
            + \sum_{m=2}^{M}\log \tilde{A}^k_{Z^n_{m}Z^n_{m+1}}
            + \sum_{m=1}^{M}\log \tilde{e}^k_{Z^n_m X^n_m}] \\
    &= - \log\lambda^k_{Z^n_{1}} -\mathbb{E}_{q(Z)}[            
            \sum_{m=2}^{M}\log \tilde{A}^k_{Z^n_{m}Z^n_{m+1}}
            + \sum_{m=1}^{M}\log \tilde{e}^k_{Z^n_m X^n_m}]
\end{align*}
Consider $q(C)$ is actually a categorical distribution and the defined relation
$\mathbb{E}_{q(C)}[c_{nk}] = r_{nk}$. Therefore, we have:
\begin{align*}
    q(C) = \prod_{n,k} r_{nk}^{c_{nk}}
\end{align*}
and 
\begin{align*}
    \log \rho_{nk} &= \psi(\tilde{\alpha_\pi}_k) - \psi(\sum_{j=1}^{K}\tilde{\alpha_\pi}_j)
                    - \tilde{H}_{nk}(q(Z)) \\
         \rho_{nk} &= \exp[\psi(\tilde{\alpha_\pi}_k) - \psi(\sum_{j=1}^{K}\tilde{\alpha_\pi}_j)
                        - \tilde{H}_{nk}(q(Z))] \\
         r_{nk} &= \frac{\rho_{nk}}{\sum_{j=1}^{K}\rho_nk}
\end{align*}
where $\rho_{nk}$ is a unnormalized posterior responsibility of class k for data sequence n and
$r_{nk}$ is a normalized posterior responsibility.

We now have an approximated posterior of the model. $p(X)$ is hard to have an analytical expression
but one can estimated its lower bound $\mathcal{L}(q)$.

We have:
\begin{align*}
    \log p(X) &= \mathrm{KL}(q||p) + \mathcal{L}(q) \\
    \mathrm{KL}(q||p) &= 
        \sum_{Z,C,\pi,\Theta} q(Z,C,\pi,\Theta) 
        \log\frac{q(Z,C,\pi,\Theta)}{p(Z,C,\pi,\Theta\mid X)} \\
    \mathcal{L}(q) &= 
        \sum_{Z,C,\pi,\Theta} q(Z,C,\pi,\Theta) 
        \log\frac{p(Z,C,\pi,\Theta, X)}{q(Z,C,\pi,\Theta)} \\
\end{align*}

When we have a good approximation of $p(Z,C,\pi,\Theta\mid X)$, $\mathrm{KL}(q||p)$
will tend to zero and $\log p(X)$ can be estimated as $\mathcal{L}(q)$. 

Therefore, we have:
\begin{align*}
    p(X) \approx \exp[\mathcal{L}(q)]
\end{align*}
\end{problem}
\pagebreak


%  ===============================================================================
% part 2.9
\begin{problem}{2.9.25}
By the graph, we can easy write down the joint probability as:
\begin{align*}
    p(M, \Theta, x) = p(\Theta)p(M)p(x \mid M, \Theta)
\end{align*}
And each term as:
\begin{align*}
    p(\Theta) &= \prod_{r \in [R], c\in [C]} p(\theta_{r,c}) \\
        &= \prod_{r \in [R], c\in [C]} \mathcal{N}(\theta_{r,c}\mid \mu_{1}, \lambda_{1}^{-1}) \\
    p(M) &= \prod_{r \in [R], c\in [C]} p(\mu_{r,c}) \\
    p(M) &= \prod_{r \in [R], c\in [C]} \mathcal{N}(\mu_{r,c}\mid \mu_{0}, \lambda_{0}^{-1}) \\
    p(x \mid M, \Theta)
        &= \prod_{n=1}^{N}\prod_{r \in [R], c\in [C]} 
                p(x^{n}_{r,c}\mid \theta_{r,c}, \mu_{r,c}, \{\mu_{i,j}: i, j \in N(r,c)\})\\
        &= \prod_{n=1}^{N}\prod_{r \in [R], c\in [C]} \mathcal{N}(x^{n}_{r,c} \mid  
            (\mu_{r,c} + \theta_{r,c}\sum_{i,j \in N(r,c)}\mu_{i,j}), \lambda^{-1})
\end{align*}
Therefore, the log joint probability is:
\begin{align*}
    \log p(M, \Theta, x) &= \log p(\Theta) + \log p(M) + \log p(x \mid M, \Theta) \\
        & = \sum_{r \in [R], c\in [C]} \log p(\theta_{r,c}, \mu_{r,c}, x^{1:N}_{r,c})
\end{align*}
where $\log p(\theta_{r,c}, \mu_{r,c}, x^{1:N}_{r,c})$ is the unnormalized log posterior
of each grid point:
\begin{align*}
    \log p(\theta_{r,c}, \mu_{r,c}, x^{1:N}_{r,c}) =& 
    \log p(\theta_{r,c}) + \log p(\mu_{r,c}) 
        + \log p(x_{r,c}\mid \theta_{r,c}, \mu_{r,c}, \{\mu_{i,j}: i, j \in N(r,c)\}) \\
    =& \frac{\log \lambda_1}{2} + \frac{\log \lambda_0}{2} + \frac{\log \lambda}{2} 
       - \frac{\lambda_0}{2}(\mu_{r,c} - \mu_0)^2 - \frac{\lambda_1}{2}(\theta_{r,c} - \mu_1)^2 \\
     & - \frac{\lambda}{2}\sum_{n=1}^{N}(x^n_{r,c} - (\mu_{r,c} + \theta_{r,c}\sum_{i,j \in N(r,c)}\mu_{i,j}))^2
       + \text{const}
\end{align*}
By mean field approximation, we can assume the form of the estimated posterior as:
\begin{align*}
    q(\Theta, M) &= q(\Theta)q(M) \\
                 &= \prod_{r \in [R], c\in [C]} q(\theta_{r,c}, \mu_{r,c}) \\
                 &= \prod_{r \in [R], c\in [C]} q(\theta_{r,c})q(\mu_{r,c})
\end{align*}
By inspection, we can assume that $q(\theta_{r,c})$ and $q(\mu_{r,c})$ are Gaussians
since the joint probability is a product of Gaussian distributions.

Therefore, we can safely consider:
\begin{align*}
    q(\theta_{r,c}) &= \mathcal{N}(\theta_{r,c} 
                \mid \tilde{m}^{\theta}_{r,c}, \tilde{\lambda^{\theta}}^{-1}_{r,c}) \\
    q(\mu_{r,c}) &= \mathcal{N}(\mu_{r,c} 
                \mid \tilde{m}^{\mu}_{r,c}, \tilde{\lambda^{\mu}}^{-1}_{r,c})
\end{align*}

It is convenient to first consider a Gaussian distribution in log space.
\begin{align*}
    z &\sim \mathcal{N}(z\mid \alpha, \beta^{-1}) \\
    \log p(z) &= -\frac{\beta}{2}(z - \alpha)^2 +\text{const} \\
    \log p(z) &= -\frac{\beta}{2}z^2 + \alpha\beta z + \text{const} \numberthis \label{eq:logspaceNormal}
\end{align*}

We get $q(\mu_{r,c})$ by averaging out all the variables except for $\mu_{r,c}$:
\begin{align*}
    \log q(\mu_{r,c}) &= 
        \mathbb{E}_{q(\Theta)q(M\backslash\mu_{r,c})}[\log p(M, \Theta, x)] 
        + \text{const} \\
    &= \mathbb{E}_{q(\theta_{r,c})q(\{\mu_{i,j}: i, j \in N(r,c)\})}[\log p(\theta_{r,c}, \mu_{r,c}, x^{1:N}_{r,c})]
    + \text{const} \numberthis \label{eq:q29_indep_asumpt} \\
    &= -\frac{\lambda_0}{2}(\mu_{r,c} - \mu_0)^2
    -\frac{\lambda}{2}\sum_n\mathbb{E}[(x^n_{r,c} - (\mu_{r,c} + \theta_{r,c}\sum_{i,j \in N(r,c)}\mu_{i,j}))^2] + \text{const} \\
    &= -\frac{\lambda_0}{2}(\mu_{r,c} - \mu_0)^2 
    -\frac{\lambda}{2}\sum_n\mathbb{E}[-2x^n_{r,c}\mu_{r,c} + \mu_{r,c}^2 + 2(\theta_{r,c}\sum_{i,j \in N(r,c)}\mu_{i,j})\mu_{r,c}]
    + \text{const} \\
    &= -\frac{\lambda_0}{2}(\mu_{r,c} - \mu_0)^2 
    -\frac{\lambda}{2}\sum_n[-2x^n_{r,c}\mu_{r,c} + \mu_{r,c}^2 + 2(\tilde{m}^{\theta}_{r,c}\sum_{i,j \in N(r,c)}\tilde{m}^{\mu}_{i,j})\mu_{r,c}]
    + \text{const} \\
    &= (-\frac{\lambda_0}{2} -\frac{N\lambda}{2})\mu_{r,c}^2
    + (\lambda_0\mu_0 + \lambda (\sum_n x^n_{r,c}) -N\lambda(\tilde{m}^{\theta}_{r,c}\sum_{i,j \in N(r,c)}\tilde{m}^{\mu}_{i,j}))\mu_{r,c}
    + \text{const} \\
\end{align*}

In eq.\eqref{eq:q29_indep_asumpt}, we do the mean field approximation and 
assume that the unnormalized posterior of each grid point only depends on
estimated values for simplicity. 

Making use of eq. \eqref{eq:logspaceNormal}, the parameters of $q(\mu_{r,c})$ are:
\begin{align*}
    -\frac{\tilde{\lambda^{\mu}}_{r,c}}{2} &= -\frac{\lambda_0}{2} -\frac{N\lambda}{2} \\
    \tilde{\lambda^{\mu}}_{r,c} &= \lambda_0 + N\lambda \\
    % mu
    \tilde{m^{\mu}}_{r,c} &= 
    \frac{1}{\tilde{\lambda^{\mu}}_{r,c}}
    (\lambda_0\mu_0 + \lambda x_{rc} -\lambda(\tilde{m}^{\theta}_{r,c}\sum_{i,j \in N(r,c)}\tilde{m}^{\mu}_{i,j})) \\
    &= \frac{1}{\lambda_0 + N\lambda}
    (\lambda_0\mu_0 + \lambda (\sum_n x^n_{r,c}) - N\lambda(\tilde{m}^{\theta}_{r,c}\sum_{i,j \in N(r,c)}\tilde{m}^{\mu}_{i,j}))
\end{align*}
Therefore, the update equations of $q(\mu_{r,c})$ are:
\begin{align}
    \tilde{\lambda^{\mu}}_{r,c}^{(t+1)} &= \lambda_0 + N\lambda \\
    % mu
    \tilde{m^{\mu}}_{r,c}^{(t+1)}  &= \frac{1}{\lambda_0 + N\lambda}
    (\lambda_0\mu_0 + \lambda (\sum_n x^n_{r,c}) - N\lambda(\tilde{m}^{\theta}_{r,c}\sum_{i,j \in N(r,c)}\tilde{m^{\mu}}_{i,j}^{(t)}))
\end{align}

We get $q(\theta_{r,c})$ by averaging out all the variables except for $\theta_{r,c}$.

For sake of convenient, we define:
\begin{equation}
    a_{r,c} = \sum_{i,j \in N(r,c)}\mu_{i,j}
\end{equation}
\begin{align*}
    \log q(\theta_{r,c}) &= 
        \mathbb{E}_{q(M)q(\Theta\backslash\theta_{r,c})}[\log p(M, \Theta, x)] 
        + \text{const} \\
    &= -\frac{\lambda_1}{2}(\theta_{r,c} - \mu_1)^2
    -\frac{\lambda}{2}\sum_n\mathbb{E}[(x^n_{r,c} - (\mu_{r,c} + \theta_{r,c}\sum_{i,j \in N(r,c)}\mu_{i,j}))^2]
    + \text{const} \\
    &= -\frac{\lambda_1}{2}(\theta_{r,c} - \mu_1)^2
    -\frac{\lambda}{2}\sum_n\mathbb{E}[\theta^2_{r,c}a_{r,c}^2 + (2\mu_{r,c}a_{r,c} - 2x^n_{r,c}a_{r,c})\theta_{r,c}]
    + \text{const} \\
    &=-\frac{\lambda_1}{2}(\theta_{r,c} - \mu_1)^2  
    -\frac{\lambda}{2}(N\theta^2_{r,c}\mathbb{E}[a_{r,c}^2] 
            + (2N\tilde{m}^{\mu}_{r,c}\mathbb{E}[a_{r,c}] - 2(\sum_n x^n_{r,c})\mathbb{E}[a_{r,c}])\theta_{r,c})
    + \text{const} \\
    &= (-\frac{\lambda_1}{2} -\frac{N\lambda\mathbb{E}[a_{r,c}^2]}{2})\theta_{r,c}^2 
    + (\lambda_1\mu_1 - N\lambda\tilde{m}^{\mu}_{r,c}\mathbb{E}[a_{r,c}] 
        + \lambda (\sum_n x^n_{r,c})\mathbb{E}[a_{r,c}])\theta_{r,c}
    + \text{const} \\
\end{align*}
Consider $a_{r,c}$ is a sum of random variables and each of them has a posterior distribution $q(\mu_{r,c})$. 
Therefore:
\begin{align*}
    \mathbb{E}_{q(M)}[a_{r,c}] &= \sum_{i,j \in N(r,c)}\tilde{m}^{\mu}_{i,j} \\
    \mathbb{E}_{q(M)}[a_{r,c}^2] &= \mathrm{Var}_{q(M)}[a_{r,c}] + \mathbb{E}_{q(M)}[a_{r,c}]^2 \\
    &= \sum_{i,j \in N(r,c)}\tilde{\lambda^{\mu}}^{-1}_{i,j} + (\sum_{i,j \in N(r,c)}\tilde{m}^{\mu}_{i,j})^2
\end{align*}

Making use of eq.\eqref{eq:logspaceNormal}, we have:
\begin{align*}
    % lambda of theta given x
    -\frac{\tilde{\lambda^{\theta}}_{r,c}}{2} 
    &= -\frac{\lambda_1}{2} -\frac{N\lambda\mathbb{E}_{q(M)}[a_{r,c}^2]}{2} \\
    \tilde{\lambda^{\theta}}_{r,c}
    &= \lambda_1 + N\lambda\sum_{i,j \in N(r,c)}(\tilde{\lambda^{\mu}}^{-1}_{i,j} + (\tilde{m}^{\mu}_{i,j})^2) \\
    % mu of theta given x
    \tilde{m^{\theta}}_{r,c} 
    &= \frac{1}{\tilde{\lambda^{\theta}}_{r,c}}
    (\lambda_1\mu_1 - N\lambda\tilde{m}^{\mu}_{r,c}\mathbb{E}_{q(M)}[a_{r,c}] 
    + \lambda (\sum_n x^n_{r,c})\mathbb{E}_{q(M)}[a_{r,c}]) \\
    &= \frac{1}{\tilde{\lambda^{\theta}}_{r,c}}
    (\lambda_1\mu_1 + (\sum_n x^n_{r,c} - N\tilde{m}^{\mu}_{r,c})\lambda\sum_{i,j \in N(r,c)}\tilde{m}^{\mu}_{i,j})
\end{align*}

Therefore, the update equations of $q(\theta_{r,c})$ are:
\begin{align*}
    % lambda of theta given x
    \tilde{\lambda^{\theta}}_{r,c}^{(t+1)}
    &= \lambda_1 + N\lambda\biggl(
        \sum_{i,j \in N(r,c)}\tilde{\lambda^{\mu}}^{-1}_{i,j} + (\sum_{i,j \in N(r,c)}\tilde{m}^{\mu}_{i,j})^2 
        \biggl)\\
    % mu of theta given x
    \tilde{m^{\theta}}_{r,c}^{(t+1)}
    &= \frac{1}{\tilde{\lambda^{\theta}}_{r,c}^{(t+1)}}
    (\lambda_1\mu_1 + (\sum_n x^n_{r,c} - N\tilde{m^{\mu}}_{r,c}^{(t)})\lambda\sum_{i,j \in N(r,c)}\tilde{m^{\mu}}_{i,j}^{(t)})
\end{align*}
\end{problem} % 2.9.25

% \begin{proof}
%     Proof goes here. Repeat as needed
% \end{proof}


% Appendix
\pagebreak
\appendix
\section{Source code of implementations}
\lstinputlisting[label={impl:Sec23},language=Python, caption=Implmentation of problem 2.3.10, 
                 basicstyle=\ttfamily\footnotesize, breaklines=true
                 label={lst:label}]
                 {src/sec2_3/main23.py}
\lstinputlisting[label={impl:Sec25},language=Python, caption=Implmentation of problem 2.5.16, 
                 basicstyle=\ttfamily\footnotesize, breaklines=true]
                 {src/sec2_5/main25.py}
\lstinputlisting[label={impl:Sec26},language=Python, caption=Implmentation of problem 2.6.19, 
                 basicstyle=\ttfamily\footnotesize, breaklines=true]
                 {src/sec2_6/main26.py}
\pagebreak
\lstinputlisting[label={impl:Sec27},language=Python, caption=Implmentation of problem 2.7.22, 
                 basicstyle=\ttfamily\footnotesize, breaklines=true]{src/sec2_7/main27.py}
% \end{appendices}
%  ===============================================================================
\end{document}
