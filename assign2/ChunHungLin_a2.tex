\documentclass[12pt]{article}

% packages
\usepackage[margin=1in]{geometry}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{subfig}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{cleveref}

\setlength{\parindent}{0em}
\setlength{\parskip}{.3em}

% \newcommand{\N}{\mathbb{N}}
% \newcommand{\Z}{\mathbb{Z}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
% \newcommand*\descriptionlabel[1]{\hspace\leftmargin$#1$}

\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
%If you want to title your bold things something different just make another thing exactly like this but replace "problem" with the name of the thing you want, like theorem or lemma or whatever
 
\begin{document}
 
%\renewcommand{\qedsymbol}{\filledbox}
%Good resources for looking up how to do stuff:
%Binary operators: http://www.access2science.com/latex/Binary.html
%General help: http://en.wikibooks.org/wiki/LaTeX/Mathematics
%Or just google stuff
 
\title{DD2434 Machine Learning, Advanced Course Assignment 2}
\author{Lin Chun Hung, chlin3@kth.se}
\maketitle

% 2.6
\begin{problem}{2.6.18}
Notices that the each observation is the sum of two Gaussian random variables.
Therefore, we can first estimated the parameters of the observation model of 
a pair of players and then solve the linear equations to get each model parameters.
\begin{align*}
    p(x_r^{(n,n')}\mid z_r^{(n,n')} = s) = \mathcal{N}(x_r^{(n,n')} \mid \mu^{n}_s + \mu^{n'}_s, 2\beta)
\end{align*}
where $z_r^{(n,n')} = s$ means the pair of player $(n, n')$ visting 
subfield $s$ at sequence index $r$ and $z$ means hidden state.

First we will derive an EM algorithm for a HMM with Gaussian observation model 
and then extend it to our case.

Consider the joint probability of a HMM where the initial state probaility and 
the transition matrix is given and the observation model is a conditional
Gaussian.
\begin{align*}
    p(z_{1:R}, x_{1:R}) &= p(z_1) \prod_{r=2}^{R}p(z_r\mid z_{r-1})
            \prod_{r=1}^{R}p(x_r\mid z_r) \\
    p(x_r\mid z_r=s) &= \mathcal{N}(x_r \mid \mu_s, \Sigma_k)
\end{align*}

The expected complete log likelihood (Q term):
\begin{align*}
    Q(\theta, \theta^{t}) &= \mathbb{E}_{p(z_{1:R}\mid x_{1:R}, \theta^{t})}[
        \log p(z_{1:R}, x_{1:R}\mid \theta)]
\end{align*}
where $\theta$ is a set of parameters for this HMM model.
\begin{align*}
    \theta = \{\bm{\pi}, \bm{A}, \bm{\mu}, \bm{\Sigma}\}
\end{align*}
where $\bm{\pi}$ is the initial state probability vector, $\bm{A}$ is the transition
matrix, $\bm{\mu} = \{ \mu_s: s \in S\}$ and $\bm{\Sigma} = \{ \Sigma_s: s \in S\}$.

\begin{align*}
    Q(\theta, \theta^{t}) &= 
    \mathbb{E}_{p(z_{1:R}\mid x_{1:R}, \theta^{t})}[
        \log p(z_1) + \sum_{r=2}^{R} \log p(z_r \mid z_{r-1}) 
        + \sum_{r=1}^{R} \log p(x_r\mid z_r)] \\
    &= \mathbb{E}[\log p(z_1)] 
        + \mathbb{E}[\sum_{r=2}^{R} \log p(z_r \mid z_{r-1})] 
        + \mathbb{E}[\sum_{r=1}^{R} \log p(x_r\mid z_r)] \\
    &= Q_{\pi}(\bm{\pi}) + Q_{A}(\bm{A}) + Q_{\mu, \Sigma}(\bm{\mu}, \bm{\Sigma})
\end{align*}
When $\bm{\pi}$ and $\bm{A}$ are given, $Q_{\pi}(\bm{\pi})$ and $Q_{A}(\bm{A})$ 
are not change in each iteration. Therefore, we only need to maximize 
$Q_{\mu, \Sigma}(\bm{\mu}, \bm{\Sigma})$ to update $\bm{\mu}$ and $\bm{\Sigma}$.

Let's go back to see the details of $Q_{\mu, \Sigma}(\bm{\mu}, \bm{\Sigma})$ 
\begin{align*}
    Q_{\mu, \Sigma}(\bm{\mu}, \bm{\Sigma}) 
    &= \mathbb{E}_{p(z_{1:R}\mid x_{1:R}, \theta^{t})}[\sum_{r=1}^{R} \log p(x_r\mid z_r)] \\
    &= \sum_{z_{1:R}}p(z_{1:R}\mid x_{1:R}, \theta^{t})\sum_{r=1}^{R} \log p(x_r\mid z_r) \\
    &= \sum_{s\in S}\sum_{r=1}^{R} p(z_{r} = s\mid x_{1:R}, \theta^{t})\log p(x_r\mid z_r = s) \\
    &= \sum_{s\in S}\sum_{r=1}^{R} \gamma_{rs}
        (-\frac{1}{2}\log 2\pi -\frac{1}{2}\log \Sigma_s - \frac{(x_r - \mu_s)^2}{2\Sigma_s})
\end{align*}
where $\gamma_{rs} = p(z_{r} = s\mid x_{1:R}, \theta^{t})$ and it can be computed
by forward-backward algorithm.

The third step is just marginalzing out unrelated variables.

In M step, we have:
\begin{align*}
    \bm{\mu}^{(t+1)} 
        &= \underset{\bm{\mu}}{\mathrm{argmin}} ~ Q_{\mu, \Sigma}(\bm{\mu}, \bm{\Sigma})  \\
    \bm{\Sigma}^{(t+1)} 
        &= \underset{\bm{\Sigma}}{\mathrm{argmin}} ~ Q_{\mu, \Sigma}(\bm{\mu}, \bm{\Sigma}) 
\end{align*}
By inspection, we can see that $Q_{\mu, \Sigma}(\bm{\mu}, \bm{\Sigma})$ 
is a convex function and therefore
we can find $\bm{\mu}^{(t+1)}$ and $\bm{\Sigma}^{(t+1)}$ 
by setting the first derivatives of $Q_{\mu, \Sigma}(\bm{\mu}, \bm{\Sigma})$
to zero.

\begin{flushleft}
    \textbf{\underline{Update $\mu_s$}}
\end{flushleft}

\begin{align*}
    \frac{\partial}{\partial \mu_s}Q_{\mu, \Sigma}(\bm{\mu}, \bm{\Sigma}) 
    &= \sum_{r=1}^{R} \gamma_{rs}(\frac{x_r - \mu_s}{\Sigma_s}) = 0 \\
    \mu_s &= \frac{\sum_{r=1}^{R}\gamma_{rs}x_r}{\sum_{r=1}^{R}\gamma_{rs}}
\end{align*}

\begin{flushleft}
    \textbf{\underline{Update $\Sigma_s$}}
\end{flushleft}

\begin{align*}
    \frac{\partial}{\partial {\Sigma_s}^{-1}}Q_{\mu, \Sigma}(\bm{\mu}, \bm{\Sigma}) 
    &= \sum_{r=1}^{R} \gamma_{rs}(\frac{1}{2{\Sigma_s}^{-1}} - \frac{1}{2}(x_r - \mu_s)^2) = 0 \\
    \Sigma_s &= \frac{\sum_{r=1}^{R}\gamma_{rs}(x_r - \mu_s)^2}{\sum_{r=1}^{R}\gamma_{rs}}
\end{align*}

For each pair of players, we estimate $\mu_s^{n,n'}$ and $\Sigma_s^{n,n'}$.
$\mu_s^{n}$ can be obtained by solving the system of linear equations.

\begin{align*}
    \beta &= \frac{\sum_{s \in S}\sum_{n, n' \in [N]}\Sigma_s^{n,n'}}{2(2M)(N^2)} \\
    &= \frac{\sum_{s \in S}\sum_{n, n' \in [N]}\Sigma_s^{n,n'}}{4 M N^2} \\
\end{align*}
\end{problem}
\pagebreak

\begin{problem}{2.8.24}
Consider the full joint distribution can be factorized as:
\begin{align*}
    p(X, Z, C, \Theta, \pi) = p(Z, C, \Theta, \pi \mid X)p(X)
\end{align*}
And the posterior $p(Z, C, \Theta, \pi \mid X)$ can be approximated by VI.

\medskip
By the graph, we can see the joint probability as:
\begin{align*}
    p(X, Z, C, \Theta, \pi) = p(X, Z \mid \Theta, C) p(C\mid \pi) p(\pi) p(\Theta)
\end{align*}
And the log probability:
\begin{align*}
    \log p(X, Z, C, \Theta, \pi) = 
    &\log p(X \mid Z, e, C) + \log p(Z \mid A, C) + \log p(C\mid \pi)  \\
    & + \log p(\pi) + \log p(A)  + \log p(e)
\end{align*}
From the question description, we can write down:
\begin{align*}
    p(\pi) &= \mathrm{Dir}(\pi_1, \pi_2, ..., \pi_K \mid \alpha_1, \alpha_2, ..., \alpha_K) \\
    p(A) &= \prod_{k,r} \mathrm{Dir}(A^k_{r,1}, A^k_{r,2}, ..., A^k_{r,R},\mid \alpha_A) \\
    p(B) &= \prod_{k,r} \mathrm{Dir}(e^k_{r,r-\Delta}, e^k_{r,r-\Delta +1}, ..., e^k_{r,r+\Delta}\mid \alpha_e) \\
    p(C\mid \pi) &= \prod_{n,k} \pi_k^{c_{nk}} \\
    p(Z\mid C, A) &= \prod_{n,k}(\lambda^k_{Z^n_{1}}
        \prod_{m=2}^{M} A^k_{Z^n_{m}Z^n_{m+1}})^{c_{nk}} \\
    p(X\mid Z, C, e) &= \prod_{n,k} (\prod_{m=1}^{M} e^k_{Z^n_{m}X^n_{m}})^{c_{nk}}
    % p(X, Z\mid \Theta, C) &= 
    %     \prod_{n,k}(\lambda^k_{Z^n_{1}}
    %     \prod_{m=2}^{M} A^k_{Z^n_{m}Z^n_{m+1}}
    %     \prod_{m=1}^{M} e^k_{Z^n_{m}X^n_{m}})^{c_{nk}}
\end{align*}
where $c_{nk}=1$ if sequence ~$X^n$ belongs to cluster k otherwise $c_{nk}=0$,
      $\lambda^k_{Z^n_{1}}$ is the initial-state probability,
      $A^k_{Z^n_{m}Z^n_{m+1}} = p(Z^n_{m+1}\mid Z^n_{m})$ is the transition probability,
  and $e^k_{Z^n_{m}X^n_{m}} = p(X^n_{m}\mid Z^n_{m})$ is the emission probability.

Assuming the following factorization.
\begin{align*}
    q(Z, C, A, e, \pi) &= q(Z)q(C)q(A)q(e)q(\pi)
\end{align*}


% Update Q(A)
\begin{flushleft}
\textbf{\underline{Update $q(A)$}}
\end{flushleft}

\begin{align*}
    \log q(A) &= \mathbb{E}_{q(Z)q(\pi)q(C)q(e)}[\log p(X, Z, C, \Theta, \pi)] + \text{const} \\
    &= \mathbb{E}_{q(Z)q(C)}[\log p(Z\mid A, C)] + \log p(A)+ \text{const} \\
    &= \mathbb{E}_{q(Z)q(C)}[\sum_{n,k}c_{nk}(\log\lambda^k_{Z^n_{1}} + \sum_{m=2}^{M}\log A^k_{Z^n_{m}Z^n_{m+1}})] 
        + \log p(A)+ \text{const} \\
    &= \sum_{n,k}\mathbb{E}_{q(C)}[c_{nk}]\mathbb{E}_{q(Z)}[\sum_{m=2}^{M}\log A^k_{Z^n_{m}Z^n_{m+1}}] 
        + \log p(A)+ \text{const} \\
\end{align*}
We look at the equation above term-by term:
\begin{align*}
    \log p(A) &= \sum_{k,r}\sum_{j=1}^{R} (\alpha_{A,j} - 1)\log A^k_{r,j} \\
    &= \sum_{k=1}^{K}\sum_{r=1}^{R}\sum_{j=1}^{R} (\alpha_{A,j} - 1)\log A^k_{r,j}
\end{align*}
\begin{align*}
    \mathbb{E}_{q(Z)}[\sum_{m=2}^{M}\log A^k_{Z^n_{m}Z^n_{m+1}}] &=
    \sum_{Z}q(Z)\sum_{m=2}^{M}\log A^k_{Z^n_{m}Z^n_{m+1}} \\
    &= \sum_{Z}q(Z)\sum_{m=2}^{M}\sum_{r=1}^{R}\sum_{j=1}^{R}\mathbb{I}(Z^n_{m}=r, Z^n_{m+1}=j)\log A^k_{r,j} \\
    &= \sum_{r=1}^{R}\sum_{j=1}^{R}\sum_{m=2}^{M}q(Z^n_{m}=r, Z^n_{m+1}=j)\log A^k_{r,j}
\end{align*}
For simplicity, define $r_{nk}$ as:
\begin{equation}
    r_{nk} = \mathbb{E}_{q(C)}[c_{nk}] 
\end{equation}
Therefore,
\begin{align*}
    &\sum_{n,k}\mathbb{E}_{q(C)}[c_{nk}]\mathbb{E}_{q(Z)}[\sum_{m=2}^{M}\log A^k_{Z^n_{m}Z^n_{m+1}}] \\
    &= \sum_{n,k} r_{nk} \sum_{r=1}^{R}\sum_{j=1}^{R}\sum_{m=2}^{M}q(Z^n_{m}=r, Z^n_{m+1}=j)\log A^k_{r,j} \\
    &= \sum_{k}\sum_{r}\sum_{j}(\sum_{n}\sum_{m=2}r_{nk} q(Z^n_{m}=r, Z^n_{m+1}=j))\log A^k_{r,j}
\end{align*}
Therefore, we can see that $q(A)$ is a Dirichlet Distribution.
\begin{align*}
    q(A) = \prod_{k,r} \mathrm{Dir}(A^k_{r,1}, A^k_{r,2}, ..., A^k_{r,R}
        \mid \tilde{\alpha_{A}}^k_{r,1},\tilde{\alpha_{A}}^k_{r,2},...,\tilde{\alpha_{A}}^k_{r,R}) \\
\end{align*}
with the parameters:
\begin{equation}
    \tilde{\alpha_{A}}^k_{r,j} = \alpha_{A,j} + \sum_{n=1}^{N}\sum_{m=2}^{M}r_{nk} q(Z^n_{m}=r, Z^n_{m+1}=j)
\end{equation}

% Update Q(e)
\begin{flushleft}
    \textbf{\underline{Update $q(e)$}}
\end{flushleft}
% Consider $q(e)$:
\begin{align*}
    \log q(e) &= \mathbb{E}_{q(Z)q(\pi)q(C)q(A)}[\log p(X, Z, C, \Theta, \pi)] + \text{const} \\
    &= \log p(e) + \mathbb{E}_{q(Z)q(C)}[\log p(X\mid Z, C, e)] + \text{const} \\
    &= \log p(e) 
        + \sum_{n,k}\mathbb{E}_{q(C)}[c_{nk}]\sum_{m=1}^{M}\mathbb{E}_{q(Z)}[\log e^k_{Z^n_{m}X^n_{m}}] 
        + \text{const} \\
\end{align*}
Term by Term:
\begin{align*}
    \log p(e) &= \sum_{k,r}\sum_{l=-\Delta}^{\Delta} (\alpha_{e,l} - 1)\log e^k_{r,r+l} \\
    &= \sum_{k=1}^{K}\sum_{r=1}^{R}\sum_{l=-\Delta}^{\Delta} (\alpha_{e,l} - 1)\log e^k_{r,r+l}
\end{align*}
We can use the same skill as what we do in calculating $q(A)$
\begin{align*}
    \mathbb{E}_{q(Z)}[\mathbb{I}(Z^n_{m}=i, X^n_{m}=r+l)] 
    &= \mathbb{E}_{q(Z)}[\mathbb{I}(Z^n_{m}=i)]\mathbb{I}(X^n_{m}=r+l) \\
    &= q(Z^n_{m}=i)\mathbb{I}(X^n_{m}=r+l)
\end{align*}
Then, we have:
\begin{align*}
    \mathbb{E}_{q(Z)}[\log e^k_{Z^n_{m}X^n_{m}}] 
    &= \mathbb{E}_{q(Z)}[\sum_{i,j}\mathbb{I}(Z^n_{m}=i, X^n_{m}=j)\log e^k_{i,j}] \\
    &= \mathbb{E}_{q(Z)}[\sum_{i,l}\mathbb{I}(Z^n_{m}=i, X^n_{m}=r+l)\log e^k_{i,r+l}] \\
    &= \sum_{i=1}^{R}\sum_{l=-\Delta}^{\Delta}q(Z^n_{m}=i)\mathbb{I}(X^n_{m}=r+l)\log e^k_{i,r+l} \\
\end{align*}
Therefore,
\begin{align*}
    &\sum_{n,k}\mathbb{E}_{q(C)}[c_{nk}]\sum_{m=1}^{M}\mathbb{E}_{q(Z)}[\log e^k_{Z^n_{m}X^n_{m}}]\\
    &= \sum_{n,k}r_{nk}\sum_{m=1}^{M}\sum_{i=1}^{R}\sum_{l=-\Delta}^{\Delta}q(Z^n_{m}=i)\mathbb{I}(X^n_{m}=r+l)\log e^k_{i,r+l} \\
    &= \sum_{k=1}^{K}\sum_{i=1}^{R}\sum_{l=-\Delta}^{\Delta}
        (\sum_{n=1}^{N}\sum_{m=1}^{M}r_{nk}q(Z^n_{m}=i)\mathbb{I}(X^n_{m}=r+l))\log e^k_{i,r+l}
\end{align*}
Obviously, $q(e)$ is also a Dirichlet distribution.
\begin{align*}
    q(e) = \prod_{k,r} \mathrm{Dir}(e^k_{r,r-\Delta}, e^k_{r,r-\Delta+1}, ..., e^k_{r,r+\Delta}
        \mid \tilde{\alpha_{e}}^k_{r,-\Delta},\tilde{\alpha_{e}}^k_{r,-\Delta+1},...,\tilde{\alpha_{e}}^k_{r,\Delta}) \\
\end{align*}
with the parameters:
\begin{equation}
    \tilde{\alpha_{e}}^k_{r,l} = \alpha_{e,l} + \sum_{n=1}^{N}\sum_{m=1}^{M}r_{nk} q(Z^n_{m}=i)\mathbb{I}(X^n_{m}=r+l)
\end{equation}

% Update Q(pi)
\begin{flushleft}
    \textbf{\underline{Update $q(\pi)$}}
\end{flushleft}

\begin{align*}
    \log q(\pi) &= \mathbb{E}_{q(Z)q(C)q(A)q(e)}[\log p(X, Z, C, \Theta, \pi)] + \text{const} \\
    &= \log p(\pi) + \mathbb{E}_{q(C)}[\log p(C \mid \pi)] + \text{const} \\
    &= \sum_{k}(\alpha_k - 1)\log \pi_k + \mathbb{E}_{q(C)}[\sum_{n,k}c_{nk}\log\pi_k] + \text{const} \\
    &= \sum_{k}(\alpha_k + \sum_{n}\mathbb{E}_{q(C)}[c_{nk}] - 1)\log \pi_k + \text{const} \\
    &= \sum_{k}(\alpha_k + \sum_{n}r_{nk} - 1)\log \pi_k + \text{const} \\
\end{align*}
Therefore, $q(\pi)$ is a Dirichlet distribution 
\begin{align*}
    q(\pi) = \mathrm{Dir}(\pi_1, \pi_2, ..., \pi_K \mid 
    \tilde{\alpha_{\pi}}_1,\tilde{\alpha_{\pi}}_2,...,\tilde{\alpha_{\pi}}_K)
\end{align*}
with parameters:
\begin{equation}
    \tilde{\alpha_{\pi}}_k = \alpha_k + \sum_{n=1}^{N}r_{nk}
\end{equation}
% Update Q(Z)
\begin{flushleft}
    \textbf{\underline{Update $q(Z)$}}
\end{flushleft}
\begin{align*}
    \log q(Z) &= \mathbb{E}_{q(\pi)q(C)q(A)q(e)}[\log p(X, Z, C, \Theta, \pi)] + \text{const} \\
    &= \mathbb{E}_{q(e)}[\log p(X \mid Z, C, e)] + \mathbb{E}_{q(A)}[\log p(Z \mid C, A)]
       + \text{const} \\
    &= \mathbb{E}_{q(e)q(C)}[\sum_{n,k}c_{nk}\sum_{m=1}^{M}\log e^k_{Z^n_m X^n_m}] \\
    &\hspace{0.5cm}  + \mathbb{E}_{q(A)q(C)}[\sum_{n,k}c_{nk}(\log\lambda^k_{Z^n_{1}} + \sum_{m=2}^{M}\log A^k_{Z^n_{m}Z^n_{m+1}})]
       + \text{const} \\
    &= \sum_{n,k}r_{nk}\sum_{m=1}^{M}\log \tilde{e}^k_{Z^n_m X^n_m}
        + \sum_{n,k}r_{nk}(\log\lambda^k_{Z^n_{1}} + \sum_{m=2}^{M}\log \tilde{A}^k_{Z^n_{m}Z^n_{m+1}})
        + \text{const} \\
    &= \sum_{n,k}r_{nk}(
        (\log\lambda^k_{Z^n_{1}} + \sum_{m=2}^{M}\log \tilde{A}^k_{Z^n_{m}Z^n_{m+1}}
        +\sum_{m=1}^{M} \log \tilde{e}^k_{Z^n_m X^n_m})
        + \text{const} \\
\end{align*}
where we define $\tilde{A}^k_{Z^n_{m}Z^n_{m+1}}$ as:
\begin{align*}
    \log \tilde{A}^k_{Z^n_{m}Z^n_{m+1}} &= \mathbb{E}_{q(A)}[\log A^k_{Z^n_{m}Z^n_{m+1}}] - \log Z_{A}\\
    &= \sum_{i,j}\mathbb{I}(Z^n_{m} = i, Z^n_{m+1}=j)\mathbb{E}_{q(A)}[\log A^k_{i,j}] - \log Z_{A}\\
    &= \sum_{i,j}\mathbb{I}(Z^n_{m} = i, Z^n_{m+1}=j)
    (\psi(\tilde{\alpha_{A}}^k_{i,j}) -\psi(\sum_{l=1}^{R}\tilde{\alpha_{A}}^k_{i,l}))- \log Z_{A}\\
    &= \psi(\tilde{\alpha_{A}}^k_{Z^n_{m},Z^n_{m+1}}) -\psi(\sum_{l=1}^{R}\tilde{\alpha_{A}}^k_{Z^n_{m},l}) - \log Z_{A}\\
    \tilde{A}^k_{Z^n_{m}Z^n_{m+1}} &= \frac{1}{Z_{A}}
    \exp[\psi(\tilde{\alpha_{A}}^k_{Z^n_{m},Z^n_{m+1}}) -\psi(\sum_{l=1}^{R}\tilde{\alpha_{A}}^k_{Z^n_{m},l})] \\
\end{align*}
Simularly, we can define $\tilde{e}^k_{Z^n_m X^n_m}$:
\begin{align*}
    % log b_tilde
    \log \tilde{e}^k_{Z^n_m X^n_m} 
    &= \psi(\tilde{\alpha_{e}}^k_{Z^n_{m},X^n_{m}}) -\psi(\sum_{l=-\Delta}^{\Delta}\tilde{\alpha_{e}}^k_{Z^n_{m},Z^n_{m}+l})
    - \log Z_{e}\\
    \tilde{e}^k_{Z^n_m X^n_m} &= \frac{1}{Z_{e}}
    \exp[\psi(\tilde{\alpha_{e}}^k_{Z^n_{m},X^n_{m}}) -\psi(\sum_{l=-\Delta}^{\Delta}\tilde{\alpha_{e}}^k_{Z^n_{m},Z^n_{m}+l})]\\
\end{align*}
where $\psi(.)$ is the digamma function as well as
$Z_{A}$ and $Z_{e}$ are normalization constants to make sure that $\tilde{A}^{k}_{i,j}$
and $\tilde{e}^{k}_{i,l}$ are stochastic matrices.

Take antilogarithm on $\log q(Z)$, we have:
\begin{align*}
    q(Z) = \frac{1}{Z_{*}}\prod_{n,k}(\lambda^k_{Z^n_{1}}
        \prod_{m=2}^{M}\tilde{A}^k_{Z^n_{m}Z^n_{m+1}}\prod_{m=1}^{M}\tilde{e}^k_{Z^n_m X^n_m})^{r_{nk}}
\end{align*}
where $Z_{*}$ is a normalization constant.

Consider:
\begin{align*}
    p(Z\mid X) &\approx q(Z) \\
    &= \frac{1}{Z_{*}}\prod_{n,k}p(Z^n_{1:M}, X^n_{1:M} \mid \tilde{A}^k, \tilde{e}^k)^{r_{nk}} \\
    &= \frac{\prod_{n,k}p(Z^n_{1:M}, X^n_{1:M} \mid \tilde{A}^k, \tilde{e}^k)^{r_{nk}}}
            {\prod_{n,k}p(X^n_{1:M} \mid \tilde{A}^k, \tilde{e}^k)^{r_{nk}}}
\end{align*}

In the above equation, we can consider that the numerator is a joint probability of
a mixture of HMMs. The dominator is the approximated data likelihood given 
the parameters $\tilde{A}$ and $\tilde{e}$.
The data likelihood can be found by forward algorithm of HMM. 

In updating $q(A)$ and $q(e)$, we can use 
the forward-backward algorithm to calculate $q(Z^n_{m}=i)$ and $q(Z^n_{m}=r, Z^n_{m+1}=j)$
because:
\begin{align*}
    q(Z^n_{m}=i) &\approx p(Z^n_{m}=i \mid X, \tilde{A}, \tilde{e}) \\
    q(Z^n_{m}=i, Z^n_{m+1}=j) &\approx p(Z^n_{m}=i, Z^n_{m+1}=j\mid X, \tilde{A}, \tilde{e})
\end{align*}

% Update q(C)
\begin{flushleft}
    \textbf{\underline{Update $q(C)$}}
\end{flushleft}
\begin{align*}
    \log q(C) &= \mathbb{E}_{q(Z)q(A)q(e)q(\pi)}[\log p(X, Z, C, \Theta, \pi)] + \text{const} \\
    &= \mathbb{E}_{q(\pi)}[\log p(C \mid \pi)]
        + \mathbb{E}_{q(Z)q(e)}[\log p(X \mid Z, C, e)]  \\
        &+ \mathbb{E}_{q(Z)q(A)}[\log p(Z \mid C, A)] + \text{const} \\
\end{align*}

The first term $\mathbb{E}_{q(\pi)}[\log p(C \mid \pi)]$:
\begin{align*}
    \mathbb{E}_{q(\pi)}[\log p(C \mid \pi)] 
    &= \mathbb{E}_{q(\pi)}[\sum_{n,k} c_{nk} \log \pi_k] \\
    &= \sum_{n,k} c_{nk}\mathbb{E}_{q(\pi)}[\log \pi_k] \\
    &= \sum_{n,k} c_{nk}(\psi(\tilde{\alpha_\pi}_k) - \psi(\sum_{j=1}^{K}\tilde{\alpha_\pi}_j)) \\
\end{align*}

The second term $\mathbb{E}_{q(Z)q(e)}[\log p(X \mid Z, C, e)]$:
\begin{align*}
    \mathbb{E}_{q(Z)q(e)}[\log p(X \mid Z, C, e)]
    &= \mathbb{E}_{q(Z)q(e)}[\sum_{n,k}c_{nk}\sum_{m=1}^{M}\log e^k_{Z^n_m X^n_m}] \\
    &= \sum_{n,k}c_{nk}\mathbb{E}_{q(Z)q(e)}[\sum_{m=1}^{M}\log e^k_{Z^n_m X^n_m}] \\
    &= \sum_{n,k}c_{nk}\mathbb{E}_{q(Z)}[\sum_{m=1}^{M}\log \tilde{e}^k_{Z^n_m X^n_m}]
\end{align*}

The third term $\mathbb{E}_{q(Z)q(A)}[\log p(Z \mid C, A)]$:
\begin{align*}
    \mathbb{E}_{q(Z)q(A)}[\log p(Z \mid C, A)]
    &= \mathbb{E}_{q(Z)q(A)}[
        \sum_{n,k}c_{nk}(\log\lambda^k_{Z^n_{1}} + \sum_{m=2}^{M}\log A^k_{Z^n_{m}Z^n_{m+1}})] \\
    &= \sum_{n,k}c_{nk}(\log\lambda^k_{Z^n_{1}} 
        + \mathbb{E}_{q(Z)q(A)}[\sum_{m=2}^{M}\log A^k_{Z^n_{m}Z^n_{m+1}}]) \\
    &= \sum_{n,k}c_{nk}(\log\lambda^k_{Z^n_{1}} 
        + \mathbb{E}_{q(Z)}[\sum_{m=2}^{M}\log \tilde{A}^k_{Z^n_{m}Z^n_{m+1}}]) \\
\end{align*}
Combining the second term and the third term, we can get an entropy-like term
of the mixture of HMMs. That term can be computed by a recursive algorithm.
Define:
\begin{align*}
    \tilde{H}_{nk}(q(Z)) &= -\mathbb{E}_{q(Z)}[
            \log\lambda^k_{Z^n_{1}} 
            + \sum_{m=2}^{M}\log \tilde{A}^k_{Z^n_{m}Z^n_{m+1}}
            + \sum_{m=1}^{M}\log \tilde{e}^k_{Z^n_m X^n_m}] \\
    &= - \log\lambda^k_{Z^n_{1}} -\mathbb{E}_{q(Z)}[            
            \sum_{m=2}^{M}\log \tilde{A}^k_{Z^n_{m}Z^n_{m+1}}
            + \sum_{m=1}^{M}\log \tilde{e}^k_{Z^n_m X^n_m}]
\end{align*}
Consider $q(C)$ is actually a categorical distribution and the defined relation
$\mathbb{E}_{q(C)}[c_{nk}] = r_{nk}$. Therefore, we have:
\begin{align*}
    q(C) = \prod_{n,k} r_{nk}^{c_{nk}}
\end{align*}
and 
\begin{align*}
    \log \rho_{nk} &= \psi(\tilde{\alpha_\pi}_k) - \psi(\sum_{j=1}^{K}\tilde{\alpha_\pi}_j)
                    - \tilde{H}_{nk}(q(Z)) \\
         \rho_{nk} &= \exp[\psi(\tilde{\alpha_\pi}_k) - \psi(\sum_{j=1}^{K}\tilde{\alpha_\pi}_j)
                        - \tilde{H}_{nk}(q(Z))] \\
         r_{nk} &= \frac{\rho_{nk}}{\sum_{j=1}^{K}\rho_nk}
\end{align*}
where $\rho_{nk}$ is a unnormalized posterior responsibility of class k for data sequence n and
$r_{nk}$ is a normalized posterior responsibility.

We now have an approximated posterior of the model. $p(X)$ is hard to have an analytical expression
but one can estimated its lower bound $\mathcal{L}(q)$.

We have:
\begin{align*}
    \log p(X) &= \mathrm{KL}(q||p) + \mathcal{L}(q) \\
    \mathrm{KL}(q||p) &= 
        \sum_{Z,C,\pi,\Theta} q(Z,C,\pi,\Theta) 
        \log\frac{q(Z,C,\pi,\Theta)}{p(Z,C,\pi,\Theta\mid X)} \\
    \mathcal{L}(q) &= 
        \sum_{Z,C,\pi,\Theta} q(Z,C,\pi,\Theta) 
        \log\frac{p(Z,C,\pi,\Theta, X)}{q(Z,C,\pi,\Theta)} \\
\end{align*}

When we have a good approximation of $p(Z,C,\pi,\Theta\mid X)$, $\mathrm{KL}(q||p)$
will tend to zero and $\log p(X)$ can be estimated as $\mathcal{L}(q)$. 

Therefore, we have:
\begin{align*}
    p(X) \approx \exp[\mathcal{L}(q)]
\end{align*}
\end{problem}
\pagebreak


%  ===============================================================================
% part 2.9
\begin{problem}{2.9.25}
By the graph, we can easy write down the joint probability as:
\begin{align*}
    p(M, \Theta, x) = p(\Theta)p(M)p(x \mid M, \Theta)
\end{align*}
And each term as:
\begin{align*}
    p(\Theta) &= \prod_{r \in [R], c\in [C]} p(\theta_{r,c}) \\
        &= \prod_{r \in [R], c\in [C]} \mathcal{N}(\theta_{r,c}\mid \mu_{1}, \lambda_{1}^{-1}) \\
    p(M) &= \prod_{r \in [R], c\in [C]} p(\mu_{r,c}) \\
    p(M) &= \prod_{r \in [R], c\in [C]} \mathcal{N}(\mu_{r,c}\mid \mu_{0}, \lambda_{0}^{-1}) \\
    p(x \mid M, \Theta) &= \prod_{r \in [R], c\in [C]} 
                p(x_{r,c}\mid \theta_{r,c}, \mu_{r,c}, \{\mu_{i,j}: i, j \in N(r,c)\})\\
        &= \prod_{r \in [R], c\in [C]} \mathcal{N}(x_{r,c} \mid  
            (\mu_{r,c} + \theta_{r,c}\sum_{i,j \in N(r,c)}\mu_{i,j}), \lambda^{-1})
\end{align*}
Therefore, the log joint probability is:
\begin{align*}
    \log p(M, \Theta, x) &= \log p(\Theta) + \log p(M) + \log p(x \mid M, \Theta) \\
        & = \sum_{r \in [R], c\in [C]} \log p(\theta_{r,c}, \mu_{r,c}, x_{r,c})
\end{align*}
,where $\log p(\theta_{r,c}, \mu_{r,c}, x_{r,c})$ is the unnormalized log posterior
of each grid point:
\begin{align*}
    \log p(\theta_{r,c}, \mu_{r,c}, x_{r,c}) =& 
    \log p(\theta_{r,c}) + \log p(\mu_{r,c}) 
        + \log p(x_{r,c}\mid \theta_{r,c}, \mu_{r,c}, \{\mu_{i,j}: i, j \in N(r,c)\}) \\
    =& \frac{\log \lambda_1}{2} + \frac{\log \lambda_0}{2} + \frac{\log \lambda}{2} 
       - \frac{\lambda_0}{2}(\mu_{r,c} - \mu_0)^2 - \frac{\lambda_1}{2}(\theta_{r,c} - \mu_1)^2 \\
     & - \frac{\lambda}{2}(x_{r,c} - (\mu_{r,c} + \theta_{r,c}\sum_{i,j \in N(r,c)}\mu_{i,j}))^2 + \text{const}
\end{align*}
By mean field approximation, we can assume the form of the estimated posterior as:
\begin{align*}
    q(\Theta, M) &= q(\Theta)q(M) \\
                 &= \prod_{r \in [R], c\in [C]} q(\theta_{r,c}, \mu_{r,c}) \\
                 &= \prod_{r \in [R], c\in [C]} q(\theta_{r,c})q(\mu_{r,c})
\end{align*}
By inspection, we can assume that $q(\theta_{r,c})$ and $q(\mu_{r,c})$ are Gaussians
since the joint probability is a product of Gaussian distributions.

Therefore, we can safely consider:
\begin{align*}
    q(\theta_{r,c}) &= \mathcal{N}(\theta_{r,c} 
                \mid \tilde{m}^{\theta}_{r,c}, \tilde{\lambda^{\theta}}^{-1}_{r,c}) \\
    q(\mu_{r,c}) &= \mathcal{N}(\mu_{r,c} 
                \mid \tilde{m}^{\mu}_{r,c}, \tilde{\lambda^{\mu}}^{-1}_{r,c})
\end{align*}

It is convenient to first consider a Gaussian distribution in log space.
\begin{align*}
    z &\sim \mathcal{N}(z\mid \alpha, \beta^{-1}) \\
    \log p(z) &= -\frac{\beta}{2}(z - \alpha)^2 +\text{const} \\
    \log p(z) &= -\frac{\beta}{2}z^2 + \alpha\beta z + \text{const} \numberthis \label{eq:logspaceNormal}
\end{align*}

We get $q(\mu_{r,c})$ by averaging out all the variables except for $\mu_{r,c}$:
\begin{align*}
    \log q(\mu_{r,c}) &= 
        \mathbb{E}_{q(\Theta)q(M\backslash\mu_{r,c})}[\log p(M, \Theta, x)] 
        + \text{const} \\
    &= \mathbb{E}_{q(\theta_{r,c})q(\{\mu_{i,j}: i, j \in N(r,c)\})}[\log p(\theta_{r,c}, \mu_{r,c}, x_{r,c})]
    + \text{const} \numberthis \label{eq:q29_indep_asumpt} \\
    &= -\frac{\lambda_0}{2}(\mu_{r,c} - \mu_0)^2
    -\frac{\lambda}{2}\mathbb{E}[(x_{r,c} - (\mu_{r,c} + \theta_{r,c}\sum_{i,j \in N(r,c)}\mu_{i,j}))^2] + \text{const} \\
    &= -\frac{\lambda_0}{2}(\mu_{r,c} - \mu_0)^2 
    -\frac{\lambda}{2}\mathbb{E}[-2x_{r,c}\mu_{r,c} + \mu_{r,c}^2 + 2(\theta_{r,c}\sum_{i,j \in N(r,c)}\mu_{i,j})\mu_{r,c}]
    + \text{const} \\
    &= -\frac{\lambda_0}{2}(\mu_{r,c} - \mu_0)^2 
    -\frac{\lambda}{2}[-2x_{r,c}\mu_{r,c} + \mu_{r,c}^2 + 2(\tilde{m}^{\theta}_{r,c}\sum_{i,j \in N(r,c)}\tilde{m}^{\mu}_{i,j})\mu_{r,c}]
    + \text{const} \\
    &= (-\frac{\lambda_0}{2} -\frac{\lambda}{2})\mu_{r,c}^2
    + (\lambda_0\mu_0 + \lambda x_{rc} -\lambda(\tilde{m}^{\theta}_{r,c}\sum_{i,j \in N(r,c)}\tilde{m}^{\mu}_{i,j}))\mu_{r,c}
    + \text{const} \\
\end{align*}

In eq.\eqref{eq:q29_indep_asumpt}, we do the mean field approximation and 
assume that the unnormalized posterior of each grid point only depends on
estimated values for simplicity. 

Making use of eq. \eqref{eq:logspaceNormal}, the parameters of $q(\mu_{r,c})$ are:
\begin{align*}
    -\frac{\tilde{\lambda^{\mu}}_{r,c}}{2} &= -\frac{\lambda_0}{2} -\frac{\lambda}{2} \\
    \tilde{\lambda^{\mu}}_{r,c} &= \lambda_0 + \lambda \\
    % mu
    \tilde{m^{\mu}}_{r,c} &= 
    \frac{1}{\tilde{\lambda^{\mu}}_{r,c}}
    (\lambda_0\mu_0 + \lambda x_{rc} -\lambda(\tilde{m}^{\theta}_{r,c}\sum_{i,j \in N(r,c)}\tilde{m}^{\mu}_{i,j})) \\
    &= \frac{1}{\lambda_0 + \lambda}
    (\lambda_0\mu_0 + \lambda x_{rc} -\lambda(\tilde{m}^{\theta}_{r,c}\sum_{i,j \in N(r,c)}\tilde{m}^{\mu}_{i,j}))
\end{align*}
Therefore, the update equations of $q(\mu_{r,c})$ are:
\begin{align}
    \tilde{\lambda^{\mu}}_{r,c}^{(t+1)} &= \lambda_0 + \lambda \\
    % mu
    \tilde{m^{\mu}}_{r,c}^{(t+1)}  &= \frac{1}{\lambda_0 + \lambda}
    (\lambda_0\mu_0 + \lambda x_{rc} -\lambda(\tilde{m}^{\theta}_{r,c}\sum_{i,j \in N(r,c)}\tilde{m^{\mu}}_{r,c}^{(t)}))
\end{align}

We get $q(\theta_{r,c})$ by averaging out all the variables except for $\theta_{r,c}$.

For sake of convenient, we define:
\begin{equation}
    a_{r,c} = \sum_{i,j \in N(r,c)}\mu_{i,j}
\end{equation}
\begin{align*}
    \log q(\theta_{r,c}) &= 
        \mathbb{E}_{q(M)q(\Theta\backslash\theta_{r,c})}[\log p(M, \Theta, x)] 
        + \text{const} \\
    &= -\frac{\lambda_1}{2}(\theta_{r,c} - \mu_1)^2
    -\frac{\lambda}{2}\mathbb{E}[(x_{r,c} - (\mu_{r,c} + \theta_{r,c}\sum_{i,j \in N(r,c)}\mu_{i,j}))^2]
    + \text{const} \\
    &= -\frac{\lambda_1}{2}(\theta_{r,c} - \mu_1)^2
    -\frac{\lambda}{2}\mathbb{E}[\theta^2_{r,c}a_{r,c}^2 + (2\mu_{r,c}a_{r,c} - 2x_{r,c}a_{r,c})\theta_{r,c}]
    + \text{const} \\
    &=-\frac{\lambda_1}{2}(\theta_{r,c} - \mu_1)^2  
    -\frac{\lambda}{2}(\theta^2_{r,c}\mathbb{E}[a_{r,c}^2] 
            + (2\tilde{m}^{\mu}_{r,c}\mathbb{E}[a_{r,c}] - 2x_{r,c}\mathbb{E}[a_{r,c}])\theta_{r,c})
    + \text{const} \\
    &= (-\frac{\lambda_1}{2} -\frac{\lambda\mathbb{E}[a_{r,c}^2]}{2})\theta_{r,c}^2 
    + (\lambda_1\mu_1 - \lambda\tilde{m}^{\mu}_{r,c}\mathbb{E}[a_{r,c}] + \lambda x_{r,c}\mathbb{E}[a_{r,c}])\theta_{r,c}
    + \text{const} \\
\end{align*}
Consider $a_{r,c}$ is a sum of random variables and each of them has a posterior distribution $q(\mu_{r,c})$. 
Therefore:
\begin{align*}
    \mathbb{E}_{q(M)}[a_{r,c}] &= \sum_{i,j \in N(r,c)}\tilde{m}^{\mu}_{i,j} \\
    \mathbb{E}_{q(M)}[a_{r,c}^2] &= \mathrm{Var}_{q(M)}[a_{r,c}] + \mathbb{E}_{q(M)}[a_{r,c}] \\
    &= \sum_{i,j \in N(r,c)}(\tilde{\lambda^{\mu}}^{-1}_{i,j} + \tilde{m}^{\mu}_{i,j})
\end{align*}

Making use of eq.\eqref{eq:logspaceNormal}, we have:
\begin{align*}
    % lambda of theta given x
    -\frac{\tilde{\lambda^{\theta}}_{r,c}}{2} 
    &= -\frac{\lambda_1}{2} -\frac{\lambda\mathbb{E}_{q(M)}[a_{r,c}^2]}{2} \\
    \tilde{\lambda^{\theta}}_{r,c}
    &= \lambda_1 + \lambda\sum_{i,j \in N(r,c)}(\tilde{\lambda^{\mu}}^{-1}_{i,j} + \tilde{m}^{\mu}_{i,j}) \\
    % mu of theta given x
    \tilde{m^{\theta}}_{r,c} 
    &= \frac{1}{\tilde{\lambda^{\theta}}_{r,c}}
    (\lambda_1\mu_1 - \lambda\tilde{m}^{\mu}_{r,c}\mathbb{E}_{q(M)}[a_{r,c}] + \lambda x_{r,c}\mathbb{E}_{q(M)}[a_{r,c}]) \\
    &= \frac{1}{\tilde{\lambda^{\theta}}_{r,c}}
    (\lambda_1\mu_1 + (x_{r,c} - \tilde{m}^{\mu}_{r,c})\lambda\sum_{i,j \in N(r,c)}\tilde{m}^{\mu}_{i,j})
\end{align*}

Therefore, the update equations of $q(\theta_{r,c})$ are:
\begin{align*}
    % lambda of theta given x
    \tilde{\lambda^{\theta}}_{r,c}^{(t+1)}
    &= \lambda_1 + \lambda\sum_{i,j \in N(r,c)}((\tilde{\lambda^{\mu}}^{(t)}_{i,j})^{-1} + \tilde{m^{\mu}}_{i,j}^{(t)}) \\
    % mu of theta given x
    \tilde{m^{\theta}}_{r,c}^{(t+1)}
    &= \frac{1}{\tilde{\lambda^{\theta}}_{r,c}}
    (\lambda_1\mu_1 + (x_{r,c} - \tilde{m^{\mu}}_{r,c}^{(t)})\lambda\sum_{i,j \in N(r,c)}\tilde{m^{\mu}}_{i,j}^{(t)})
\end{align*}
\end{problem} % 2.9.25

% \begin{proof}
%     Proof goes here. Repeat as needed
% \end{proof}

%  ===============================================================================
\end{document}
