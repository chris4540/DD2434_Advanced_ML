\documentclass[12pt]{article}

% packages
\usepackage[margin=1in]{geometry}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{subfig}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{cleveref}

% \newcommand{\N}{\mathbb{N}}
% \newcommand{\Z}{\mathbb{Z}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
% \newcommand*\descriptionlabel[1]{\hspace\leftmargin$#1$}

\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
%If you want to title your bold things something different just make another thing exactly like this but replace "problem" with the name of the thing you want, like theorem or lemma or whatever
 
\begin{document}
 
%\renewcommand{\qedsymbol}{\filledbox}
%Good resources for looking up how to do stuff:
%Binary operators: http://www.access2science.com/latex/Binary.html
%General help: http://en.wikibooks.org/wiki/LaTeX/Mathematics
%Or just google stuff
 
\title{DD2434 Machine Learning, Advanced Course Assignment 2}
\author{Lin Chun Hung, chlin3@kth.se}
\maketitle

\begin{problem}{2.8.24}
By the graph, we can see the joint probability as:
\begin{align*}
    p(X, Z, C, \Theta, \pi) = p(X, Z \mid \Theta, C) p(C\mid \pi) p(\pi) p(\Theta)
\end{align*}
And the log probability:
\begin{align*}
    \log p(X, Z, C, \Theta, \pi) = 
    &\log p(X \mid Z, e, C) + \log p(Z \mid A, C) + \log p(C\mid \pi)  \\
    & + \log p(\pi) + \log p(A)  + \log p(e)
\end{align*}
From the question description, we can write down:
\begin{align*}
    p(\pi) &= \mathrm{Dir}(\pi_1, \pi_2, ..., \pi_K \mid \alpha_1, \alpha_2, ..., \alpha_K) \\
    p(A) &= \prod_{k,r} \mathrm{Dir}(A^k_{r,1}, A^k_{r,2}, ..., A^k_{r,R},\mid \alpha_A) \\
    p(B) &= \prod_{k,r} \mathrm{Dir}(e^k_{r,r-\Delta}, e^k_{r,r-\Delta +1}, ..., e^k_{r,r+\Delta}\mid \alpha_e) \\
    p(C\mid \pi) &= \prod_{n,k} \pi_k^{c_{nk}} \\
    p(Z\mid C, A) &= \prod_{n,k}(\lambda^k_{Z^n_{1}}
        \prod_{m=2}^{M} A^k_{Z^n_{m}Z^n_{m+1}})^{c_{nk}} \\
    p(X\mid Z, C, e) &= \prod_{n,k} (\prod_{m=1}^{M} e^k_{Z^n_{m}X^n_{m}})^{c_{nk}}
    % p(X, Z\mid \Theta, C) &= 
    %     \prod_{n,k}(\lambda^k_{Z^n_{1}}
    %     \prod_{m=2}^{M} A^k_{Z^n_{m}Z^n_{m+1}}
    %     \prod_{m=1}^{M} e^k_{Z^n_{m}X^n_{m}})^{c_{nk}}
\end{align*}
where $c_{nk}=1$ if sequence ~$X^n$ belongs to cluster k otherwise $c_{nk}=0$,
      $\lambda^k_{Z^n_{1}}$ is the initial-state probability,
      $A^k_{Z^n_{m}Z^n_{m+1}} = p(Z^n_{m+1}\mid Z^n_{m})$ is the transition probability,
  and $e^k_{Z^n_{m}X^n_{m}} = p(X^n_{m}\mid Z^n_{m})$ is the emission probability.

Assuming the following factorization.
\begin{align*}
    q(X, Z, C, A, e, \pi) = q(X, Z)q(C)q(A)q(e)q(\pi)
\end{align*}


% Update Q(A)
\begin{flushleft}
\textbf{\underline{Update $q(A)$}}
\end{flushleft}

\begin{align*}
    \log q(A) &= \mathbb{E}_{q(Z)q(X)q(\pi)q(C)q(e)}[\log p(X, Z, C, \Theta, \pi)] + \text{const} \\
    &= \mathbb{E}_{q(Z)q(C)}[\log p(Z\mid A, C)] + \log p(A)+ \text{const} \\
    &= \mathbb{E}_{q(Z)q(C)}[\sum_{n,k}c_{nk}(\lambda^k_{Z^n_{1}}\sum_{m=2}^{M}\log A^k_{Z^n_{m}Z^n_{m+1}})] 
        + \log p(A)+ \text{const} \\
    &= \sum_{n,k}\mathbb{E}_{q(C)}[c_{nk}]\mathbb{E}_{q(Z)}[\sum_{m=2}^{M}\log A^k_{Z^n_{m}Z^n_{m+1}}] 
        + \log p(A)+ \text{const} \\
\end{align*}
We look the equation above term-by term:
\begin{align*}
    \log p(A) &= \sum_{k,r}\sum_{j=1}^{R} (\alpha_{A,j} - 1)\log A^k_{r,j} \\
    &= \sum_{k=1}^{K}\sum_{r=1}^{R}\sum_{j=1}^{R} (\alpha_{A,j} - 1)\log A^k_{r,j}
\end{align*}
\begin{align*}
    \mathbb{E}_{q(Z)}[\sum_{m=2}^{M}\log A^k_{Z^n_{m}Z^n_{m+1}}] &=
    \sum_{Z}q(Z)\sum_{m=2}^{M}\log A^k_{Z^n_{m}Z^n_{m+1}} \\
    &= \sum_{Z}q(Z)\sum_{m=2}^{M}\sum_{r=1}^{R}\sum_{j=1}^{R}\mathbb{I}(Z^n_{m}=r, Z^n_{m+1}=j)\log A^k_{r,j} \\
    &= \sum_{r=1}^{R}\sum_{j=1}^{R}\sum_{m=2}^{M}q(Z^n_{m}=r, Z^n_{m+1}=j)\log A^k_{r,j}
\end{align*}
For simplicity, define $r_{nk}$ as:
\begin{equation}
    r_{nk} = \mathbb{E}_{q(C)}[c_{nk}] 
\end{equation}
Therefore,
\begin{align*}
    &\sum_{n,k}\mathbb{E}_{q(C)}[c_{nk}]\mathbb{E}_{q(Z)}[\sum_{m=2}^{M}\log A^k_{Z^n_{m}Z^n_{m+1}}] \\
    &= \sum_{n,k} r_{nk} \sum_{r=1}^{R}\sum_{j=1}^{R}\sum_{m=2}^{M}q(Z^n_{m}=r, Z^n_{m+1}=j)\log A^k_{r,j} \\
    &= \sum_{k}\sum_{r}\sum_{j}(\sum_{n}\sum_{m=2}r_{nk} q(Z^n_{m}=r, Z^n_{m+1}=j))\log A^k_{r,j}
\end{align*}
Therefore, we can see that $q(A)$ is a Dirichlet Distribution.
\begin{align*}
    q(A) = \prod_{k,r} \mathrm{Dir}(A^k_{r,1}, A^k_{r,2}, ..., A^k_{r,R}
        \mid \tilde{\alpha_{A}}^k_{r,1},\tilde{\alpha_{A}}^k_{r,2},...,\tilde{\alpha_{A}}^k_{r,R}) \\
\end{align*}
with the parameters:
\begin{equation}
    \tilde{\alpha_{A}}^k_{r,j} = \alpha_{A,j} + \sum_{n=1}^{N}\sum_{m=2}^{M}r_{nk} q(Z^n_{m}=r, Z^n_{m+1}=j)
\end{equation}

% Update Q(e)
\begin{flushleft}
    \textbf{\underline{Update $q(e)$}}
\end{flushleft}
% Consider $q(e)$:
\begin{align*}
    \log q(e) &= \mathbb{E}_{q(Z)q(X)q(\pi)q(C)q(A)}[\log p(X, Z, C, \Theta, \pi)] + \text{const} \\
    &= \log p(e) + \mathbb{E}_{q(Z)q(X)q(C)}[\log p(X\mid Z, C, e)] + \text{const} \\
    &= \log p(e) 
        + \sum_{n,k}\mathbb{E}_{q(C)}[c_{nk}]\sum_{m=1}^{M}\mathbb{E}_{q(Z)q(X)}[\log e^k_{Z^n_{m}X^n_{m}}] 
        + \text{const} \\
\end{align*}
Term by Term:
\begin{align*}
    \log p(e) &= \sum_{k,r}\sum_{l=-\Delta}^{\Delta} (\alpha_{e,l} - 1)\log e^k_{r,r+l} \\
    &= \sum_{k=1}^{K}\sum_{r=1}^{R}\sum_{l=-\Delta}^{\Delta} (\alpha_{e,l} - 1)\log e^k_{r,r+l}
\end{align*}
We can use the same skill as what we do in calculating $q(A)$
\begin{align*}
    \mathbb{E}_{q(Z)q(X)}[\mathbb{I}(Z^n_{m}=i, X^n_{m}=r+l)] = q(Z^n_{m}=i, X^n_{m}=r+l)
\end{align*}
Then, we have:
\begin{align*}
    \mathbb{E}_{q(Z)q(X)}[\log e^k_{Z^n_{m}X^n_{m}}] 
    &= \mathbb{E}_{q(Z)q(X)}[\sum_{i,j}\mathbb{I}(Z^n_{m}=i, X^n_{m}=j)\log e^k_{i,j}] \\
    &= \mathbb{E}_{q(Z)q(X)}[\sum_{i,l}\mathbb{I}(Z^n_{m}=i, X^n_{m}=r+l)\log e^k_{i,r+l}] \\
    &= \sum_{i=1}^{R}\sum_{l=-\Delta}^{\Delta}q(Z^n_{m}=i, X^n_{m}=r+l)\log e^k_{i,r+l} \\
\end{align*}
Therefore,
\begin{align*}
    &\sum_{n,k}\mathbb{E}_{q(C)}[c_{nk}]\sum_{m=1}^{M}\mathbb{E}_{q(Z)q(X)}[\log e^k_{Z^n_{m}X^n_{m}}]\\
    &= \sum_{n,k}r_{nk}\sum_{m=1}^{M}\sum_{i=1}^{R}\sum_{l=-\Delta}^{\Delta}q(Z^n_{m}=i, X^n_{m}=r+l)\log e^k_{i,r+l} \\
    &= \sum_{k=1}^{K}\sum_{i=1}^{R}\sum_{l=-\Delta}^{\Delta}
        (\sum_{n=1}^{N}\sum_{m=1}^{M}r_{nk}q(Z^n_{m}=i, X^n_{m}=r+l))\log e^k_{i,r+l}
\end{align*}
Obviously, $q(e)$ is also a Dirichlet distribution.
\begin{align*}
    q(e) = \prod_{k,r} \mathrm{Dir}(e^k_{r,-\Delta}, e^k_{r,-\Delta+1}, ..., e^k_{r,\Delta}
        \mid \tilde{\alpha_{e}}^k_{r,-\Delta},\tilde{\alpha_{e}}^k_{r,-\Delta+1},...,\tilde{\alpha_{e}}^k_{r,\Delta}) \\
\end{align*}
with the parameters:
\begin{equation}
    \tilde{\alpha_{e}}^k_{r,l} = \alpha_{e,l} + \sum_{n=1}^{N}\sum_{m=1}^{M}r_{nk} q(Z^n_{m}=r, X^n_{m}=r+l)
\end{equation}

% Update Q(pi)
\begin{flushleft}
    \textbf{\underline{Update $q(\pi)$}}
\end{flushleft}

\begin{align*}
    \log q(\pi) &= \mathbb{E}_{q(Z,X)q(e)q(C)q(A)q(e)}[\log p(X, Z, C, \Theta, \pi)] + \text{const} \\
    &= \log p(\pi) + \mathbb{E}_{q(C)}[\log p(C \mid \pi)] + \text{const} \\
    &= \sum_{k}(\alpha_k - 1)\log \pi_k + \mathbb{E}_{q(C)}[\sum_{n,k}c_{nk}\log\pi_k] + \text{const} \\
    &= \sum_{k}(\alpha_k + \sum_{n}\mathbb{E}_{q(C)}[c_{nk}] - 1)\log \pi_k + \text{const} \\
    &= \sum_{k}(\alpha_k + \sum_{n}r_{nk} - 1)\log \pi_k + \text{const} \\
\end{align*}
Therefore, $q(\pi)$ is a Dirichlet distribution 
\begin{align*}
    q(\pi) = \mathrm{Dir}(\pi_1, \pi_2, ..., \pi_K \mid 
    \tilde{\alpha_{\pi}}_1,\tilde{\alpha_{\pi}}_2,...,\tilde{\alpha_{\pi}}_K)
\end{align*}
with parameters:
\begin{equation}
    \tilde{\alpha_{\pi}}_k = \alpha_k + \sum_{n=1}^{N}r_{nk}
\end{equation}
\end{problem}
\pagebreak


%  ===============================================================================
% part 2.9
\begin{problem}{2.9.25}
By the graph, we can easy write down the joint probability as:
\begin{align*}
    p(M, \Theta, x) = p(\Theta)p(M)p(x \mid M, \Theta)
\end{align*}
And each term as:
\begin{align*}
    p(\Theta) &= \prod_{r \in [R], c\in [C]} p(\theta_{r,c}) \\
        &= \prod_{r \in [R], c\in [C]} \mathcal{N}(\theta_{r,c}\mid \mu_{1}, \lambda_{1}^{-1}) \\
    p(M) &= \prod_{r \in [R], c\in [C]} p(\mu_{r,c}) \\
    p(M) &= \prod_{r \in [R], c\in [C]} \mathcal{N}(\mu_{r,c}\mid \mu_{0}, \lambda_{0}^{-1}) \\
    p(x \mid M, \Theta) &= \prod_{r \in [R], c\in [C]} 
                p(x_{r,c}\mid \theta_{r,c}, \mu_{r,c}, \{\mu_{i,j}: i, j \in N(r,c)\})\\
        &= \prod_{r \in [R], c\in [C]} \mathcal{N}(x_{r,c} \mid  
            (\mu_{r,c} + \theta_{r,c}\sum_{i,j \in N(r,c)}\mu_{i,j}), \lambda^{-1})
\end{align*}
Therefore, the log joint probability is:
\begin{align*}
    \log p(M, \Theta, x) &= \log p(\Theta) + \log p(M) + \log p(x \mid M, \Theta) \\
        & = \sum_{r \in [R], c\in [C]} \log p(\theta_{r,c}, \mu_{r,c}, x_{r,c})
\end{align*}
,where $\log p(\theta_{r,c}, \mu_{r,c}, x_{r,c})$ is the unnormalized log posterior
of each grid point:
\begin{align*}
    \log p(\theta_{r,c}, \mu_{r,c}, x_{r,c}) =& 
    \log p(\theta_{r,c}) + \log p(\mu_{r,c}) 
        + \log p(x_{r,c}\mid \theta_{r,c}, \mu_{r,c}, \{\mu_{i,j}: i, j \in N(r,c)\}) \\
    =& \frac{\log \lambda_1}{2} + \frac{\log \lambda_0}{2} + \frac{\log \lambda}{2} 
       - \frac{\lambda_0}{2}(\mu_{r,c} - \mu_0)^2 - \frac{\lambda_1}{2}(\theta_{r,c} - \mu_1)^2 \\
     & - \frac{\lambda}{2}(x_{r,c} - (\mu_{r,c} + \theta_{r,c}\sum_{i,j \in N(r,c)}\mu_{i,j}))^2 + \text{const}
\end{align*}
By mean field approximation, we can assume the form of the estimated posterior as:
\begin{align*}
    q(\Theta, M) &= q(\Theta)q(M) \\
                 &= \prod_{r \in [R], c\in [C]} q(\theta_{r,c}, \mu_{r,c}) \\
                 &= \prod_{r \in [R], c\in [C]} q(\theta_{r,c})q(\mu_{r,c})
\end{align*}
By inspection, we can assume that $q(\theta_{r,c})$ and $q(\mu_{r,c})$ are Gaussians
since the joint probability is a product of Gaussian distributions.

Therefore, we can safely consider:
\begin{align*}
    q(\theta_{r,c}) &= \mathcal{N}(\theta_{r,c} 
                \mid \tilde{m}^{\theta}_{r,c}, \tilde{\lambda^{\theta}}^{-1}_{r,c}) \\
    q(\mu_{r,c}) &= \mathcal{N}(\mu_{r,c} 
                \mid \tilde{m}^{\mu}_{r,c}, \tilde{\lambda^{\mu}}^{-1}_{r,c})
\end{align*}

It is convenient to first consider a Gaussian distribution in log space.
\begin{align*}
    z &\sim \mathcal{N}(z\mid \alpha, \beta^{-1}) \\
    \log p(z) &= -\frac{\beta}{2}(z - \alpha)^2 +\text{const} \\
    \log p(z) &= -\frac{\beta}{2}z^2 + \alpha\beta z + \text{const} \numberthis \label{eq:logspaceNormal}
\end{align*}

We get $q(\mu_{r,c})$ by averaging out all the variables except for $\mu_{r,c}$:
\begin{align*}
    \log q(\mu_{r,c}) &= 
        \mathbb{E}_{q(\Theta)q(M\backslash\mu_{r,c})}[\log p(M, \Theta, x)] 
        + \text{const} \\
    &= \mathbb{E}_{q(\theta_{r,c})q(\{\mu_{i,j}: i, j \in N(r,c)\})}[\log p(\theta_{r,c}, \mu_{r,c}, x_{r,c})]
    + \text{const} \numberthis \label{eq:q29_indep_asumpt} \\
    &= -\frac{\lambda_0}{2}(\mu_{r,c} - \mu_0)^2
    -\frac{\lambda}{2}\mathbb{E}[(x_{r,c} - (\mu_{r,c} + \theta_{r,c}\sum_{i,j \in N(r,c)}\mu_{i,j}))^2] + \text{const} \\
    &= -\frac{\lambda_0}{2}(\mu_{r,c} - \mu_0)^2 
    -\frac{\lambda}{2}\mathbb{E}[-2x_{r,c}\mu_{r,c} + \mu_{r,c}^2 + 2(\theta_{r,c}\sum_{i,j \in N(r,c)}\mu_{i,j})\mu_{r,c}]
    + \text{const} \\
    &= -\frac{\lambda_0}{2}(\mu_{r,c} - \mu_0)^2 
    -\frac{\lambda}{2}[-2x_{r,c}\mu_{r,c} + \mu_{r,c}^2 + 2(\tilde{m}^{\theta}_{r,c}\sum_{i,j \in N(r,c)}\tilde{m}^{\mu}_{i,j})\mu_{r,c}]
    + \text{const} \\
    &= (-\frac{\lambda_0}{2} -\frac{\lambda}{2})\mu_{r,c}^2
    + (\lambda_0\mu_0 + \lambda x_{rc} -\lambda(\tilde{m}^{\theta}_{r,c}\sum_{i,j \in N(r,c)}\tilde{m}^{\mu}_{i,j}))\mu_{r,c}
    + \text{const} \\
\end{align*}

In eq.\eqref{eq:q29_indep_asumpt}, we do the mean field approximation and 
assume that the unnormalized posterior of each grid point only depends on
estimated values for simplicity. 

Making use of eq. \eqref{eq:logspaceNormal}, the parameters of $q(\mu_{r,c})$ are:
\begin{align*}
    -\frac{\tilde{\lambda^{\mu}}_{r,c}}{2} &= -\frac{\lambda_0}{2} -\frac{\lambda}{2} \\
    \tilde{\lambda^{\mu}}_{r,c} &= \lambda_0 + \lambda \\
    % mu
    \tilde{m^{\mu}}_{r,c} &= 
    \frac{1}{\tilde{\lambda^{\mu}}_{r,c}}
    (\lambda_0\mu_0 + \lambda x_{rc} -\lambda(\tilde{m}^{\theta}_{r,c}\sum_{i,j \in N(r,c)}\tilde{m}^{\mu}_{i,j})) \\
    &= \frac{1}{\lambda_0 + \lambda}
    (\lambda_0\mu_0 + \lambda x_{rc} -\lambda(\tilde{m}^{\theta}_{r,c}\sum_{i,j \in N(r,c)}\tilde{m}^{\mu}_{i,j}))
\end{align*}
Therefore, the update equations of $q(\mu_{r,c})$ are:
\begin{align}
    \tilde{\lambda^{\mu}}_{r,c}^{(t+1)} &= \lambda_0 + \lambda \\
    % mu
    \tilde{m^{\mu}}_{r,c}^{(t+1)}  &= \frac{1}{\lambda_0 + \lambda}
    (\lambda_0\mu_0 + \lambda x_{rc} -\lambda(\tilde{m}^{\theta}_{r,c}\sum_{i,j \in N(r,c)}\tilde{m^{\mu}}_{r,c}^{(t)}))
\end{align}

We get $q(\theta_{r,c})$ by averaging out all the variables except for $\theta_{r,c}$.

For sake of convenient, we define:
\begin{equation}
    a_{r,c} = \sum_{i,j \in N(r,c)}\mu_{i,j}
\end{equation}
\begin{align*}
    \log q(\theta_{r,c}) &= 
        \mathbb{E}_{q(M)q(\Theta\backslash\theta_{r,c})}[\log p(M, \Theta, x)] 
        + \text{const} \\
    &= -\frac{\lambda_1}{2}(\theta_{r,c} - \mu_1)^2
    -\frac{\lambda}{2}\mathbb{E}[(x_{r,c} - (\mu_{r,c} + \theta_{r,c}\sum_{i,j \in N(r,c)}\mu_{i,j}))^2]
    + \text{const} \\
    &= -\frac{\lambda_1}{2}(\theta_{r,c} - \mu_1)^2
    -\frac{\lambda}{2}\mathbb{E}[\theta^2_{r,c}a_{r,c}^2 + (2\mu_{r,c}a_{r,c} - 2x_{r,c}a_{r,c})\theta_{r,c}]
    + \text{const} \\
    &=-\frac{\lambda_1}{2}(\theta_{r,c} - \mu_1)^2  
    -\frac{\lambda}{2}(\theta^2_{r,c}\mathbb{E}[a_{r,c}^2] 
            + (2\tilde{m}^{\mu}_{r,c}\mathbb{E}[a_{r,c}] - 2x_{r,c}\mathbb{E}[a_{r,c}])\theta_{r,c})
    + \text{const} \\
    &= (-\frac{\lambda_1}{2} -\frac{\lambda\mathbb{E}[a_{r,c}^2]}{2})\theta_{r,c}^2 
    + (\lambda_1\mu_1 - \lambda\tilde{m}^{\mu}_{r,c}\mathbb{E}[a_{r,c}] + \lambda x_{r,c}\mathbb{E}[a_{r,c}])\theta_{r,c}
    + \text{const} \\
\end{align*}
Consider $a_{r,c}$ is a sum of random variables and each of them has a posterior distribution $q(\mu_{r,c})$. 
Therefore:
\begin{align*}
    \mathbb{E}_{q(M)}[a_{r,c}] &= \sum_{i,j \in N(r,c)}\tilde{m}^{\mu}_{i,j} \\
    \mathbb{E}_{q(M)}[a_{r,c}^2] &= \mathrm{Var}_{q(M)}[a_{r,c}] + \mathbb{E}_{q(M)}[a_{r,c}] \\
    &= \sum_{i,j \in N(r,c)}(\tilde{\lambda^{\mu}}^{-1}_{i,j} + \tilde{m}^{\mu}_{i,j})
\end{align*}

Making use of eq.\eqref{eq:logspaceNormal}, we have:
\begin{align*}
    % lambda of theta given x
    -\frac{\tilde{\lambda^{\theta}}_{r,c}}{2} 
    &= -\frac{\lambda_1}{2} -\frac{\lambda\mathbb{E}_{q(M)}[a_{r,c}^2]}{2} \\
    \tilde{\lambda^{\theta}}_{r,c}
    &= \lambda_1 + \lambda\sum_{i,j \in N(r,c)}(\tilde{\lambda^{\mu}}^{-1}_{i,j} + \tilde{m}^{\mu}_{i,j}) \\
    % mu of theta given x
    \tilde{m^{\theta}}_{r,c} 
    &= \frac{1}{\tilde{\lambda^{\theta}}_{r,c}}
    (\lambda_1\mu_1 - \lambda\tilde{m}^{\mu}_{r,c}\mathbb{E}_{q(M)}[a_{r,c}] + \lambda x_{r,c}\mathbb{E}_{q(M)}[a_{r,c}]) \\
    &= \frac{1}{\tilde{\lambda^{\theta}}_{r,c}}
    (\lambda_1\mu_1 + (x_{r,c} - \tilde{m}^{\mu}_{r,c})\lambda\sum_{i,j \in N(r,c)}\tilde{m}^{\mu}_{i,j})
\end{align*}

Therefore, the update equations of $q(\theta_{r,c})$ are:
\begin{align*}
    % lambda of theta given x
    \tilde{\lambda^{\theta}}_{r,c}^{(t+1)}
    &= \lambda_1 + \lambda\sum_{i,j \in N(r,c)}((\tilde{\lambda^{\mu}}^{(t)}_{i,j})^{-1} + \tilde{m^{\mu}}_{i,j}^{(t)}) \\
    % mu of theta given x
    \tilde{m^{\theta}}_{r,c}^{(t+1)}
    &= \frac{1}{\tilde{\lambda^{\theta}}_{r,c}}
    (\lambda_1\mu_1 + (x_{r,c} - \tilde{m^{\mu}}_{r,c}^{(t)})\lambda\sum_{i,j \in N(r,c)}\tilde{m^{\mu}}_{i,j}^{(t)})
\end{align*}
\end{problem} % 2.9.25

% \begin{proof}
%     Proof goes here. Repeat as needed
% \end{proof}

%  ===============================================================================
\end{document}
