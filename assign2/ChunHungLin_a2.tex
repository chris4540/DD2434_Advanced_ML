\documentclass[12pt]{article}

% packages
\usepackage[margin=1in]{geometry}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{subfig}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{cleveref}
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
 
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
%If you want to title your bold things something different just make another thing exactly like this but replace "problem" with the name of the thing you want, like theorem or lemma or whatever
 
\begin{document}
 
%\renewcommand{\qedsymbol}{\filledbox}
%Good resources for looking up how to do stuff:
%Binary operators: http://www.access2science.com/latex/Binary.html
%General help: http://en.wikibooks.org/wiki/LaTeX/Mathematics
%Or just google stuff
 
\title{DD2434 Machine Learning, Advanced Course Assignment 2}
\author{Lin Chun Hung, chlin3@kth.se}
\maketitle

%  ===============================================================================
% part 2.9
\begin{problem}{2.9.25}
By the graph, we can easy write down the joint probability as:
\begin{align*}
    p(M, \Theta, x) = p(\Theta)p(M)p(x \mid M, \Theta)
\end{align*}
And each term as:
\begin{align*}
    p(\Theta) &= \prod_{r \in [R], c\in [C]} p(\theta_{r,c}) \\
        &= \prod_{r \in [R], c\in [C]} \mathcal{N}(\theta_{r,c}\mid \mu_{1}, \lambda_{1}) \\
    p(M) &= \prod_{r \in [R], c\in [C]} p(\mu_{r,c}) \\
    p(M) &= \prod_{r \in [R], c\in [C]} \mathcal{N}(\mu_{r,c}\mid \mu_{0}, \lambda_{0}) \\
    p(x \mid M, \Theta) &= \prod_{r \in [R], c\in [C]} 
                p(x_{r,c}\mid \theta_{r,c}, \mu_{r,c}, \{\mu_{i,j}: i, j \in N(r,c)\})\\
        &= \prod_{r \in [R], c\in [C]} \mathcal{N}(x_{r,c} \mid  
            (\mu_{r,c} + \theta_{r,c}\sum_{i,j \in N(r,c)}\mu_{i,j}), \lambda)
\end{align*}
Therefore, the log joint probability is:
\begin{align*}
    \log p(M, \Theta, x) &= \log p(\Theta) + \log p(M) + \log p(x \mid M, \Theta) \\
        & = \sum_{r \in [R], c\in [C]} \log p(\theta_{r,c}, \mu_{r,c}, x_{r,c})
\end{align*}
,where $\log p(\theta_{r,c}, \mu_{r,c}, x_{r,c})$ is the unnormalized log posterior
of each grid point:
\begin{align*}
    \log p(\theta_{r,c}, \mu_{r,c}, x_{r,c}) =& 
    \log p(\theta_{r,c}) + \log p(\mu_{r,c}) 
        + \log p(x_{r,c}\mid \theta_{r,c}, \mu_{r,c}, \{\mu_{i,j}: i, j \in N(r,c)\}) \\
    =& -\frac{\log \lambda_1}{2} -\frac{\log \lambda_0}{2} -\frac{\log \lambda}{2} 
       - \frac{(\mu_{r,c} - \mu_0)^2}{2\lambda_0} - \frac{(\theta_{r,c} - \mu_1)^2}{2\lambda_1} \\
     & - \frac{(x_{r,c} - (\mu_{r,c} + \theta_{r,c}\sum_{i,j \in N(r,c)}\mu_{i,j}))}{2\lambda}
\end{align*}
\end{problem}

% \begin{proof}
%     Proof goes here. Repeat as needed
% \end{proof}

%  ===============================================================================
\end{document}
