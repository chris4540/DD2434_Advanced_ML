% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
 
\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,bm}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{subfig}
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
 
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{reflection}[2][Reflection]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\kern10pt
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

% \newcommand*{\answer}{%
%   \par
%   \kern1pt
%   \begingroup
%     \centering
%     \raisebox{.2\baselineskip}{%
%       \textcolor{gray}{
% 	    \rule{.6667\linewidth}{.1pt}%
%       }
%     }%
%     \par
%   \kern8pt
%   \endgroup
% }
\newcommand\TODO[1]{\textcolor{red}{#1}}

\begin{document}
 
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
 
%\renewcommand{\qedsymbol}{\filledbox}
 
\title{DD2434 Machine Learning, Advanced Course Assignment 1}
\author{Lin Chun Hung, chlin3@kth.se} 
 
\maketitle


% Question 1
\begin{question}{1}
% TODO: Fix the arguments
Consider input output pairs are linked by the mapping to have the following
 relation:
$$ 
    \bf{t}_i = f(\bf{x}_i) + \bm{\epsilon}
$$
where the $\bm{\epsilon}$ is the unbiased random noise. Since we have no piror knowledge
on the random noise term, the random noise is then assumed as following the normal
distribution. TODO: More on this \\

With the assumption that features are uncorrelated, we choose the spherical
covariance matrix for the likelihood.
\end{question}
% End question 2



% Question 2
\begin{question}{2}
Consider the general product rule of probability:
$$\mathrm {P} \left(\bigcap _{k=1}^{n}A_{k}\right)=
  \prod _{k=1}^{n}\mathrm {P} \left(A_{k}\,{\Bigg |}\,\bigcap _{j=1}^{k-1}A_{j}\right)$$

Therefore the likelihood would be:
$$ 
  p(\bf{T}\mid f,\bf{X}) =
  \prod _{i=1}^{N}p(\bf{t}_i \mid \bf{t}_{i-1},...,\bf{t}_{1},
  f,\bf{X})
$$

\end{question} 
% End question 2


% Question 3 
\begin{question}{3}

$$ 
  p(\bf{T}\mid \bf{X},\bf{W}) =
  \prod _{i=1}^{N}N(\bf{W}\bf{x}_i, \sigma^2\bf{I})
$$

\end{question}
% End question 3

% Question 4
\begin{question}{4}
The choice of piror distribution reflects the choice of regularizer. Chossing L1 
norm as regularizer is known as the lasso (least absolute shrinkage and selection operator).
The regularizer will force some weighting coefficients $w_j$ to be zero, given 
a proper choice of model parameter $\tau$. It plays the role of feature selection
since those zero weighting coefficients indicate that the corresponding features
are irrelvant to the output. \\ 

The penalization term or the negative log-prior:

$$  \frac{\lambda}{2} \left \|\textbf{vec}(\bf{W} - \bf{W}_0)\right \|_{q}  $$

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\linewidth]{q4_regular.png}
  \caption{A boat.}
  \label{fig:boat1}
\end{figure}

TODO: Discussion
\end{question}
% End question 4



% Question 5
\begin{question}{5}vec()
Consider the regression equation in general:
$$\textbf{T} = \textbf{WX} + \textbf{ErrorMatrix} $$
Therefore, the likelihood in terms of matrix normal distribution is:
\begin{align*}
p(\bf{T}\mid \bf{X}, \bf{W}) &= 
  \mathcal{NM}_{D \times N}(\bf{WX}, \bf{I}, \sigma^2 \bf{I}) \\ 
  &= \mathcal{N}_{DN}(\bf{vec}(\bf{WX}), \sigma^2 \bf{I})
\end{align*}
The prior is:
\begin{align*}
p(\bf{W}) &= 
  \mathcal{NM}_{D \times q}(\bf{W}_0, \bf{I}, \tau^2 \bf{I})
  &= \mathcal{N}_{Dq}(\bf{vec}(\bf{W}), \tau^2 \bf{I})
\end{align*}

\end{question}
% End question 5
% --------------------------------------------------------------
%     You don't have to mess with anything below this line.
% --------------------------------------------------------------
 
\end{document}
